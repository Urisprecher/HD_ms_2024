{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178511a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##import libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix \n",
    "%matplotlib inline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.manifold import TSNE\n",
    "from colour import Color\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "import sklearn.linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import svm\n",
    "from sklearn.datasets import make_moons, make_blobs\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix \n",
    "from psynlig import plot_correlation_heatmap\n",
    "import pingouin as pg\n",
    "from scipy.stats import pearsonr\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "from plotly.tools import FigureFactory as FF\n",
    "import chart_studio as cs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import chart_studio.plotly as py\n",
    "from plotly.offline import iplot\n",
    "import pandas as pd\n",
    "import numpy as np  \n",
    "%matplotlib inline  \n",
    "from plotly import __version__ \n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import cufflinks as cf\n",
    "init_notebook_mode(connected=True)\n",
    "cf.go_offline()\n",
    "import chart_studio.plotly as py\n",
    "from sklearn import preprocessing\n",
    "from pandas.plotting import scatter_matrix \n",
    "from psynlig import plot_correlation_heatmap\n",
    "import pingouin as pg\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52c6916",
   "metadata": {},
   "outputs": [],
   "source": [
    "##functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51737ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_dataframe(df, group_col):\n",
    "\n",
    "    grouped_df = df.groupby(group_col)\n",
    "   \n",
    "\n",
    "    agg_dict = {}\n",
    "   \n",
    "\n",
    "    for col in df.columns:\n",
    "        if col != group_col:\n",
    "            agg_dict[col] = [\n",
    "                ('count', 'count'),\n",
    "                ('mean', 'mean'),\n",
    "                ('std', 'std'),\n",
    "                ('min', 'min'),\n",
    "                ('25%', lambda x: np.quantile(x, 0.25)),\n",
    "                ('median', 'median'),\n",
    "                ('75%', lambda x: np.quantile(x, 0.75)),\n",
    "                ('max', 'max')\n",
    "                \n",
    "            ]\n",
    "   \n",
    "\n",
    "    summary_df = grouped_df.agg(agg_dict)\n",
    "   \n",
    "\n",
    "    summary_df.columns = [f'{col}_{stat}' for col, stat in summary_df.columns]\n",
    "   \n",
    "\n",
    "    summary_df.index.name = group_col\n",
    "   \n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8469b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_plate_column(folder_path):\n",
    "    files = os.listdir(folder_path)\n",
    "    output_folder = os.path.join(folder_path, \"output\")\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "   \n",
    "    for i, file in enumerate(files):\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            output_path = os.path.join(output_folder, file)\n",
    "           \n",
    "            df = pd.read_csv(file_path)\n",
    "            df.insert(0, \"plate\", f\"p{i+1}\")\n",
    "            df['Row'] = 'r' + df['Row'].astype(str)\n",
    "            df['Column'] = 'c' + df['Column'].astype(str)\n",
    "            df[\"pos\"] = df[\"Row\"].astype(str) + df[\"Column\"].astype(str)  + df[\"plate\"].astype(str) \n",
    "            df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e900e7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_exp_column_to_csv_files(input_path, output_path):\n",
    "\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "   \n",
    "\n",
    "    csv_files = [file for file in os.listdir(input_path) if file.endswith('.csv')]\n",
    "   \n",
    "    for file in csv_files:\n",
    "\n",
    "        file_path = os.path.join(input_path, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "       \n",
    "\n",
    "        exp_value = file[7:10] \n",
    "       \n",
    "\n",
    "        df['EXP'] = exp_value\n",
    "       \n",
    "\n",
    "        output_file_path = os.path.join(output_path, file)\n",
    "        df.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b80c2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = ('RESULTS_mito_markers_feat_seelction') \n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "    print('Output folder created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc16f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DRP1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e7941d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = glob.glob('noam_may_analysis_fold1/DRP1/*.csv')\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    li.append(df)\n",
    "\n",
    "drp1_df = pd.concat(li, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb21c28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove treated data for feat select\n",
    "cell_types =  drp1_df[\"Compound\"].values\n",
    "sns.histplot(cell_types)\n",
    "plt.show()\n",
    "print(f'Total number of cell types {len(cell_types)}')\n",
    "drp1_df = drp1_df.loc[cell_types != 'mdivi1 25uM']\n",
    "sns.histplot(drp1_df[\"Compound\"].values)\n",
    "plt.show()\n",
    "print(f'Total number of cells {len(drp1_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054cd6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##define groups \n",
    "df_h = drp1_df[(drp1_df[\"Severity\"] == 'HC')] \n",
    "df_s = drp1_df[(drp1_df[\"Severity\"] == 'S')]\n",
    "frame_drp = (df_h, df_s)\n",
    "frame_drp_f = pd.concat(frame_drp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b627e091",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MiguelW12\\AppData\\Local\\Temp\\ipykernel_9812\\3460966092.py:5: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  CLEAN_dfheatmapREAL_drp = HEAT_DATA_drp1.groupby(level=1).mean()\n"
     ]
    }
   ],
   "source": [
    "HEAT_DATA_drp1 = frame_drp_f.copy()\n",
    "HEAT_DATA_drp1[\"group_with_id\"] = HEAT_DATA_drp1[\"Cell ID\"].astype(str) + HEAT_DATA_drp1[\"Cell Type\"].astype(str) \n",
    "HEAT_DATA_drp1.set_index([\"group_with_id\"], inplace = True,\n",
    "                            append = True, drop = False)\n",
    "CLEAN_dfheatmapREAL_drp = HEAT_DATA_drp1.groupby(level=1).mean()\n",
    "\n",
    "CLEAN_dfheatmap_final_drp = CLEAN_dfheatmapREAL_drp.drop(columns=['Row', 'Plane', 'Timepoint', 'Number of Analyzed Fields', 'Height [µm]',\n",
    "                            'Time [s]', 'Column', \"Cell ID\"])\n",
    "CLEAN_dfheatmap_final_drp.fillna(0, inplace=True)\n",
    "CLEAN_dfheatmap_final_NORMAL_drp = CLEAN_dfheatmap_final_drp.copy()\n",
    "CLEAN_dfheatmap_final_NORMAL_drp=(CLEAN_dfheatmap_final_NORMAL_drp-CLEAN_dfheatmap_final_NORMAL_drp.mean())/CLEAN_dfheatmap_final_NORMAL_drp.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cde5c1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = 'mito_drp1_cor.csv'\n",
    "CLEAN_dfheatmap_final_NORMAL_drp.to_csv(file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c6b767",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_drp_f = frame_drp_f.drop(columns=['Row', 'Plane', 'Timepoint', 'Number of Analyzed Fields', 'Height [µm]',\n",
    "                            'Time [s]', 'Column', 'Compound', \"Cell Type\", \"Cell ID\"])\n",
    "\n",
    "frame_drp_f = frame_drp_f.rename(columns={\"Severity\":\"group\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ecbcac08",
   "metadata": {},
   "outputs": [],
   "source": [
    "##sum data\n",
    "frame_drp_f = frame_drp_f.dropna()\n",
    "summary_df = summarize_dataframe(frame_drp_f,'group')\n",
    "file2 = 'summary_mito_drp.csv'\n",
    "summary_df.to_csv(file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "24b31318",
   "metadata": {},
   "outputs": [],
   "source": [
    "file3 = 'mito_drp1_combined.csv'\n",
    "frame_drp_f.to_csv(file3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfa104c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##PCA\n",
    "PCA_DATA = frame_drp_f.copy()\n",
    "PCA_DATA.fillna(0, inplace=True)\n",
    "features2 = PCA_DATA.columns[:54]  \n",
    "x2 = PCA_DATA.loc[:, features2].values\n",
    "y2 = PCA_DATA.loc[:,['group']].values\n",
    "x2 = StandardScaler().fit_transform(x2)\n",
    "pd.DataFrame(data = x2, columns = features2).head()\n",
    "pca2 = PCA(n_components=2)\n",
    "principalComponents2 = pca2.fit_transform(x2)\n",
    "PCA_DATA2_COMP = pd.DataFrame(data = principalComponents2\n",
    "             , columns = ['principal component 1', 'principal component 2'])\n",
    "importances2 = pd.DataFrame(data = abs(pca2.components_).transpose(), columns = ['PC1', 'PC2'])\n",
    "importances2['Feature'] = features2\n",
    "display(importances2.sort_values('PC1', ascending = False).reset_index(drop = True))\n",
    "imp2 = importances2.sort_values('PC2', ascending = False)[0:30]\n",
    "\n",
    "ax2 = sns.barplot(x = 'PC2', y = 'Feature', data = imp2)\n",
    "fig = ax2.get_figure()\n",
    "fig.set_size_inches(4, 8)\n",
    "ax2.set_xticklabels(ax2.get_xticklabels(), rotation = 90)  \n",
    "plt.show()\n",
    "    \n",
    "imp4 = importances2.sort_values('PC1', ascending = False)[0:30]\n",
    "ax2 = sns.barplot(x = 'PC1', y = 'Feature', data = imp4)\n",
    "\n",
    "fig = ax2.get_figure()\n",
    "fig.set_size_inches(4, 8)\n",
    "ax2.tick_params(axis='x', labelsize=6)\n",
    "ax2.tick_params(axis='y', labelsize=6)\n",
    "ax2.set_xticklabels(ax2.get_xticklabels(), rotation = 90)  \n",
    "plt.show()\n",
    "    \n",
    "PCA_DATA2_FINAL = pd.concat([PCA_DATA2_COMP, PCA_DATA[['group']]], axis = 1)\n",
    "PCA_FIG2 = plt.figure(figsize = (8,8))\n",
    "ax2 = PCA_FIG2.add_subplot(1,1,1) \n",
    "ax2.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax2.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax2.set_title('2 Component PCA', fontsize = 20)\n",
    "ax2.tick_params(axis='x', labelsize=6)\n",
    "ax2.tick_params(axis='y', labelsize=6)\n",
    "for tick in ax2.get_xticklabels():\n",
    "    tick.set_fontname(\"Arial\")\n",
    "for tick in ax2.get_yticklabels():\n",
    "    tick.set_fontname(\"Arial\")\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "targets2 = ['HC', 'S']\n",
    "colors2 = ['b', 'r']\n",
    "for target, color in zip(targets2,colors2):\n",
    "    indicesToKeep = PCA_DATA2_FINAL['group'] == target\n",
    "    ax2.scatter(PCA_DATA2_FINAL.loc[indicesToKeep, 'principal component 1']\n",
    "               , PCA_DATA2_FINAL.loc[indicesToKeep, 'principal component 2']\n",
    "               , c = color\n",
    "               , s = 50)\n",
    "    ax2.legend(targets2)\n",
    "    ax2.grid()\n",
    "\n",
    "print(pca2.explained_variance_ratio_)\n",
    "plt.savefig(path + '//' + 'drp1_PCA2.pdf', dpi = 300)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce7fa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA\n",
    "PCA_DATA_CLEAN = frame_drp_f.copy()\n",
    "\n",
    "x = PCA_DATA_CLEAN.loc[:, features2].values\n",
    "y = PCA_DATA_CLEAN.loc[:,['group']].values\n",
    "y = np.array([int('HC' == y_i[0][:2]) for y_i in y])\n",
    "\n",
    "y[0]=2\n",
    "x = StandardScaler().fit_transform(x)\n",
    "\n",
    "lda = LDA(n_components=2)\n",
    "lda_x = np.array(lda.fit(x, y).transform(x))\n",
    "\n",
    "\n",
    "importances = pd.DataFrame(data = abs(lda.scalings_), columns = ['PC1', 'PC2'])\n",
    "importances['Feature'] = features2\n",
    "display(importances.sort_values('PC1', ascending = False).reset_index(drop = True))\n",
    "imp = importances.sort_values('PC2', ascending = False)[0:30]\n",
    "ax1 = sns.barplot(x = 'PC2', y = 'Feature', data = imp)\n",
    "    \n",
    "fig = ax1.get_figure()\n",
    "fig.set_size_inches(4, 8)\n",
    "plt.rc('ytick', labelsize=10)\n",
    "ax1.set_xticklabels(ax1.get_xticklabels(), rotation = 90)  \n",
    "plt.show()\n",
    "imp2 = importances.sort_values('PC1', ascending = False)[0:30]\n",
    "ax2 = sns.barplot(x = 'PC1', y = 'Feature', data = imp2)\n",
    "fig = ax2.get_figure()\n",
    "fig.set_size_inches(4, 8)\n",
    "plt.rc('ytick', labelsize=10)\n",
    "ax2.set_xticklabels(ax2.get_xticklabels(), rotation = 90) \n",
    "plt.show()\n",
    "\n",
    "font = {'family' : 'Arial',\n",
    "        'size'   : 22}\n",
    "\n",
    "plt.rc('font', **font)\n",
    "\n",
    "scale=6\n",
    "target_names= ['S', 'HC']\n",
    "plt.figure()\n",
    "plt.figure(figsize = (8,8))\n",
    "for c, i, target_name in zip(\"rb\", [0, 1], target_names):\n",
    "    plt.scatter(lda_x[y == i,0] , lda_x[y == i,1] , c=c, label=target_name)\n",
    "    plt.legend(target_names*scale,fontsize=12)\n",
    "    plt.xlabel('LD1', fontsize = 20, family = 'Arial')\n",
    "    plt.ylabel('LD2', fontsize = 20, family = 'Arial')\n",
    "    plt.yticks(family = 'Arial', size = 10)\n",
    "    plt.xticks(family = 'Arial', size= 10)\n",
    "print(lda.explained_variance_ratio_)            \n",
    "plt.savefig(path + '\\\\' + 'drp-LDA.pdf', dpi=600 )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed41435",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VAT1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de18af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = glob.glob('noam_may_analysis_fold1/VAT1/*.csv')\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    li.append(df)\n",
    "\n",
    "vat1_df = pd.concat(li, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b3002c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_types =  vat1_df[\"Compound\"].values\n",
    "sns.histplot(cell_types)\n",
    "plt.show()\n",
    "print(f'Total number of cell types {len(cell_types)}')\n",
    "vat1_df = vat1_df.loc[cell_types != 'mdivi1 25uM']\n",
    "sns.histplot(vat1_df[\"Compound\"].values)\n",
    "plt.show()\n",
    "print(f'Total number of cells {len(vat1_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4de7ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_h = vat1_df[(vat1_df[\"Severity\"] == 'HC')] \n",
    "df_s = vat1_df[(vat1_df[\"Severity\"] == 'S')]\n",
    "frame_vat = (df_h, df_s)\n",
    "frame_vat_f = pd.concat(frame_vat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a8d8a900",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MiguelW12\\AppData\\Local\\Temp\\ipykernel_9812\\2454812790.py:5: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  CLEAN_dfheatmapREAL_vat1 = HEAT_DATA_vat1.groupby(level=1).mean()\n"
     ]
    }
   ],
   "source": [
    "HEAT_DATA_vat1 = frame_vat_f.copy()\n",
    "HEAT_DATA_vat1[\"group_with_id\"] = HEAT_DATA_vat1[\"Cell ID\"].astype(str) + HEAT_DATA_vat1[\"Cell Type\"].astype(str) \n",
    "HEAT_DATA_vat1.set_index([\"group_with_id\"], inplace = True,\n",
    "                            append = True, drop = False)\n",
    "CLEAN_dfheatmapREAL_vat1 = HEAT_DATA_vat1.groupby(level=1).mean()\n",
    "\n",
    "CLEAN_dfheatmap_final_vat1 = CLEAN_dfheatmapREAL_vat1.drop(columns=['Row', 'Plane', 'Timepoint', 'Number of Analyzed Fields', 'Height [µm]',\n",
    "                            'Time [s]', 'Column', \"Cell ID\"])\n",
    "CLEAN_dfheatmap_final_vat1.fillna(0, inplace=True)\n",
    "CLEAN_dfheatmap_final_NORMAL_vat1 = CLEAN_dfheatmap_final_vat1.copy()\n",
    "CLEAN_dfheatmap_final_NORMAL_vat1=(CLEAN_dfheatmap_final_NORMAL_vat1-CLEAN_dfheatmap_final_NORMAL_vat1.mean())/CLEAN_dfheatmap_final_NORMAL_vat1.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "70b93433",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = 'mito_vat1_cor.csv'\n",
    "CLEAN_dfheatmap_final_NORMAL_vat1.to_csv(file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4732c812",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_vat_f = frame_vat_f.drop(columns=['Row', 'Plane', 'Timepoint', 'Number of Analyzed Fields', 'Height [µm]',\n",
    "                            'Time [s]', 'Column', 'Compound', \"Cell Type\", \"Cell ID\"])\n",
    "\n",
    "frame_vat_f = frame_vat_f.rename(columns={\"Severity\":\"group\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "805f889b",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_vat_f = frame_vat_f.dropna()\n",
    "summary_df = summarize_dataframe(frame_vat_f,'group')\n",
    "file1 = 'summary_mito_vat.csv'\n",
    "summary_df.to_csv(file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e199faf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = 'mito_vat1_combined.csv'\n",
    "frame_vat_f.to_csv(file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31c48c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_DATA = frame_vat_f.copy()\n",
    "PCA_DATA.fillna(0, inplace=True)\n",
    "features2 = PCA_DATA.columns[:54]  \n",
    "x2 = PCA_DATA.loc[:, features2].values\n",
    "y2 = PCA_DATA.loc[:,['group']].values\n",
    "x2 = StandardScaler().fit_transform(x2)\n",
    "pd.DataFrame(data = x2, columns = features2).head()\n",
    "pca2 = PCA(n_components=2)\n",
    "principalComponents2 = pca2.fit_transform(x2)\n",
    "PCA_DATA2_COMP = pd.DataFrame(data = principalComponents2\n",
    "             , columns = ['principal component 1', 'principal component 2'])\n",
    "importances2 = pd.DataFrame(data = abs(pca2.components_).transpose(), columns = ['PC1', 'PC2'])\n",
    "importances2['Feature'] = features2\n",
    "display(importances2.sort_values('PC1', ascending = False).reset_index(drop = True))\n",
    "imp2 = importances2.sort_values('PC2', ascending = False)[0:30]\n",
    "\n",
    "ax2 = sns.barplot(x = 'PC2', y = 'Feature', data = imp2)\n",
    "fig = ax2.get_figure()\n",
    "fig.set_size_inches(4, 8)\n",
    "ax2.set_xticklabels(ax2.get_xticklabels(), rotation = 90)  \n",
    "plt.show()\n",
    "    \n",
    "imp4 = importances2.sort_values('PC1', ascending = False)[0:30]\n",
    "ax2 = sns.barplot(x = 'PC1', y = 'Feature', data = imp4)\n",
    "fig = ax2.get_figure()\n",
    "fig.set_size_inches(4, 8)\n",
    "ax2.tick_params(axis='x', labelsize=6)\n",
    "ax2.tick_params(axis='y', labelsize=6)\n",
    "ax2.set_xticklabels(ax2.get_xticklabels(), rotation = 90)  \n",
    "plt.show()\n",
    "    \n",
    "PCA_DATA2_FINAL = pd.concat([PCA_DATA2_COMP, PCA_DATA[['group']]], axis = 1)\n",
    "PCA_FIG2 = plt.figure(figsize = (8,8))\n",
    "ax2 = PCA_FIG2.add_subplot(1,1,1) \n",
    "ax2.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax2.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax2.set_title('2 Component PCA', fontsize = 20)\n",
    "ax2.tick_params(axis='x', labelsize=6)\n",
    "ax2.tick_params(axis='y', labelsize=6)\n",
    "for tick in ax2.get_xticklabels():\n",
    "    tick.set_fontname(\"Arial\")\n",
    "for tick in ax2.get_yticklabels():\n",
    "    tick.set_fontname(\"Arial\")\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "targets2 = ['HC', 'S']\n",
    "colors2 = ['b', 'r']\n",
    "for target, color in zip(targets2,colors2):\n",
    "    indicesToKeep = PCA_DATA2_FINAL['group'] == target\n",
    "    ax2.scatter(PCA_DATA2_FINAL.loc[indicesToKeep, 'principal component 1']\n",
    "               , PCA_DATA2_FINAL.loc[indicesToKeep, 'principal component 2']\n",
    "               , c = color\n",
    "               , s = 50)\n",
    "    ax2.legend(targets2)\n",
    "    ax2.grid()\n",
    "\n",
    "    \n",
    "print(pca2.explained_variance_ratio_)\n",
    "plt.savefig(path + '//' + 'vat1_PCA2.pdf', dpi = 300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985df03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##LDA\n",
    "\n",
    "PCA_DATA_CLEAN = frame_vat_f.copy()\n",
    "\n",
    "x = PCA_DATA_CLEAN.loc[:, features2].values\n",
    "y = PCA_DATA_CLEAN.loc[:,['group']].values\n",
    "y = np.array([int('HC' == y_i[0][:2]) for y_i in y])\n",
    "\n",
    "y[0]=2\n",
    "x = StandardScaler().fit_transform(x)\n",
    "\n",
    "lda = LDA(n_components=2)\n",
    "lda_x = np.array(lda.fit(x, y).transform(x))\n",
    "\n",
    "\n",
    "importances = pd.DataFrame(data = abs(lda.scalings_), columns = ['PC1', 'PC2'])\n",
    "importances['Feature'] = features2\n",
    "display(importances.sort_values('PC1', ascending = False).reset_index(drop = True))\n",
    "imp = importances.sort_values('PC2', ascending = False)[0:30]\n",
    "ax1 = sns.barplot(x = 'PC2', y = 'Feature', data = imp)\n",
    "    \n",
    "fig = ax1.get_figure()\n",
    "fig.set_size_inches(4, 8)\n",
    "plt.rc('ytick', labelsize=10)\n",
    "ax1.set_xticklabels(ax1.get_xticklabels(), rotation = 90)  \n",
    "plt.show()\n",
    "imp2 = importances.sort_values('PC1', ascending = False)[0:30]\n",
    "ax2 = sns.barplot(x = 'PC1', y = 'Feature', data = imp2)\n",
    "fig = ax2.get_figure()\n",
    "fig.set_size_inches(4, 8)\n",
    "plt.rc('ytick', labelsize=10)\n",
    "ax2.set_xticklabels(ax2.get_xticklabels(), rotation = 90) \n",
    "plt.show()\n",
    "\n",
    "font = {'family' : 'Arial',\n",
    "        'size'   : 22}\n",
    "\n",
    "plt.rc('font', **font)\n",
    "\n",
    "scale=6\n",
    "target_names= ['S', 'HC']\n",
    "plt.figure()\n",
    "plt.figure(figsize = (8,8))\n",
    "for c, i, target_name in zip(\"rb\", [0, 1], target_names):\n",
    "    plt.scatter(lda_x[y == i,0] , lda_x[y == i,1] , c=c, label=target_name)\n",
    "    plt.legend(target_names*scale,fontsize=12)\n",
    "    plt.xlabel('LD1', fontsize = 20, family = 'Arial')\n",
    "    plt.ylabel('LD2', fontsize = 20, family = 'Arial')\n",
    "    plt.yticks(family = 'Arial', size = 10)\n",
    "    plt.xticks(family = 'Arial', size= 10)\n",
    "print(lda.explained_variance_ratio_)            \n",
    "plt.savefig(path + '\\\\' + 'vat-LDA.pdf', dpi=600 )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44e8295",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MFN1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e14753d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = glob.glob('noam_may_analysis_fold1/MFN1/*.csv')\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    li.append(df)\n",
    "\n",
    "mfn1_df = pd.concat(li, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffe9d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_types =  mfn1_df[\"Compound\"].values\n",
    "sns.histplot(cell_types)\n",
    "plt.show()\n",
    "print(f'Total number of cell types {len(cell_types)}')\n",
    "mfn1_df = mfn1_df.loc[cell_types != 'mdivi1 25uM']\n",
    "sns.histplot(mfn1_df[\"Compound\"].values)\n",
    "plt.show()\n",
    "print(f'Total number of cells {len(mfn1_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaceb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_h = mfn1_df[(mfn1_df[\"Severity\"] == 'HC')] \n",
    "df_s = mfn1_df[(mfn1_df[\"Severity\"] == 'S')]\n",
    "frame_mfn1 = (df_h, df_s)\n",
    "frame_mfn1_f = pd.concat(frame_mfn1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "567a956b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MiguelW12\\AppData\\Local\\Temp\\ipykernel_9812\\1051816466.py:5: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  CLEAN_dfheatmapREAL_mfn1 = HEAT_DATA_mfn1.groupby(level=1).mean()\n"
     ]
    }
   ],
   "source": [
    "HEAT_DATA_mfn1 = frame_mfn1_f.copy()\n",
    "HEAT_DATA_mfn1[\"group_with_id\"] = HEAT_DATA_mfn1[\"Cell ID\"].astype(str) + HEAT_DATA_mfn1[\"Cell Type\"].astype(str) \n",
    "HEAT_DATA_mfn1.set_index([\"group_with_id\"], inplace = True,\n",
    "                            append = True, drop = False)\n",
    "CLEAN_dfheatmapREAL_mfn1 = HEAT_DATA_mfn1.groupby(level=1).mean()\n",
    "\n",
    "CLEAN_dfheatmap_final_mfn1 = CLEAN_dfheatmapREAL_mfn1.drop(columns=['Row', 'Plane', 'Timepoint', 'Number of Analyzed Fields', 'Height [µm]',\n",
    "                            'Time [s]', 'Column', \"Cell ID\"])\n",
    "CLEAN_dfheatmap_final_mfn1.fillna(0, inplace=True)\n",
    "CLEAN_dfheatmap_final_NORMAL_mfn1 = CLEAN_dfheatmap_final_mfn1.copy()\n",
    "CLEAN_dfheatmap_final_NORMAL_mfn1=(CLEAN_dfheatmap_final_NORMAL_mfn1-CLEAN_dfheatmap_final_NORMAL_mfn1.mean())/CLEAN_dfheatmap_final_NORMAL_mfn1.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0abd2c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = 'mito_mfn1_cor.csv'\n",
    "CLEAN_dfheatmap_final_NORMAL_mfn1.to_csv(file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9d44f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_mfn1_f = frame_mfn1_f.drop(columns=['Row', 'Plane', 'Timepoint', 'Number of Analyzed Fields', 'Height [µm]',\n",
    "                            'Time [s]', 'Column', 'Compound', \"Cell Type\", \"Cell ID\"])\n",
    "\n",
    "frame_mfn1_f = frame_mfn1_f.rename(columns={\"Severity\":\"group\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ebdb3224",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_mfn1_f = frame_mfn1_f.dropna()\n",
    "summary_df = summarize_dataframe(frame_mfn1_f,'group')\n",
    "file1 = 'summary_mito_mfn1.csv'\n",
    "summary_df.to_csv(file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5ea5fa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = 'mito_mfn1_combined.csv'\n",
    "frame_mfn1_f.to_csv(file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3245ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA\n",
    "PCA_DATA = frame_mfn1_f.copy()\n",
    "PCA_DATA.fillna(0, inplace=True)\n",
    "features2 = PCA_DATA.columns[:54]  \n",
    "x2 = PCA_DATA.loc[:, features2].values\n",
    "y2 = PCA_DATA.loc[:,['group']].values\n",
    "x2 = StandardScaler().fit_transform(x2)\n",
    "pd.DataFrame(data = x2, columns = features2).head()\n",
    "pca2 = PCA(n_components=2)\n",
    "principalComponents2 = pca2.fit_transform(x2)\n",
    "PCA_DATA2_COMP = pd.DataFrame(data = principalComponents2\n",
    "             , columns = ['principal component 1', 'principal component 2'])\n",
    "importances2 = pd.DataFrame(data = abs(pca2.components_).transpose(), columns = ['PC1', 'PC2'])\n",
    "importances2['Feature'] = features2\n",
    "display(importances2.sort_values('PC1', ascending = False).reset_index(drop = True))\n",
    "imp2 = importances2.sort_values('PC2', ascending = False)[0:30]\n",
    "\n",
    "ax2 = sns.barplot(x = 'PC2', y = 'Feature', data = imp2)\n",
    "fig = ax2.get_figure()\n",
    "fig.set_size_inches(4, 8)\n",
    "ax2.set_xticklabels(ax2.get_xticklabels(), rotation = 90)  \n",
    "plt.show()\n",
    "    \n",
    "imp4 = importances2.sort_values('PC1', ascending = False)[0:30]\n",
    "ax2 = sns.barplot(x = 'PC1', y = 'Feature', data = imp4)\n",
    "\n",
    "fig = ax2.get_figure()\n",
    "fig.set_size_inches(4, 8)\n",
    "ax2.tick_params(axis='x', labelsize=6)\n",
    "ax2.tick_params(axis='y', labelsize=6)\n",
    "ax2.set_xticklabels(ax2.get_xticklabels(), rotation = 90)  \n",
    "plt.show()\n",
    "    \n",
    "PCA_DATA2_FINAL = pd.concat([PCA_DATA2_COMP, PCA_DATA[['group']]], axis = 1)\n",
    "PCA_FIG2 = plt.figure(figsize = (8,8))\n",
    "ax2 = PCA_FIG2.add_subplot(1,1,1) \n",
    "ax2.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax2.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax2.set_title('2 Component PCA', fontsize = 20)\n",
    "ax2.tick_params(axis='x', labelsize=6)\n",
    "ax2.tick_params(axis='y', labelsize=6)\n",
    "for tick in ax2.get_xticklabels():\n",
    "    tick.set_fontname(\"Arial\")\n",
    "for tick in ax2.get_yticklabels():\n",
    "    tick.set_fontname(\"Arial\")\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "targets2 = ['HC', 'S']\n",
    "colors2 = ['b', 'r']\n",
    "for target, color in zip(targets2,colors2):\n",
    "    indicesToKeep = PCA_DATA2_FINAL['group'] == target\n",
    "    ax2.scatter(PCA_DATA2_FINAL.loc[indicesToKeep, 'principal component 1']\n",
    "               , PCA_DATA2_FINAL.loc[indicesToKeep, 'principal component 2']\n",
    "               , c = color\n",
    "               , s = 50)\n",
    "    ax2.legend(targets2)\n",
    "    ax2.grid()\n",
    "\n",
    "    \n",
    "print(pca2.explained_variance_ratio_)\n",
    "plt.savefig(path + '//' + 'mfn1_PCA2.pdf', dpi = 300)\n",
    "plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8ada14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA\n",
    "PCA_DATA_CLEAN = frame_mfn1_f.copy()\n",
    "\n",
    "x = PCA_DATA_CLEAN.loc[:, features2].values\n",
    "y = PCA_DATA_CLEAN.loc[:,['group']].values\n",
    "y = np.array([int('HC' == y_i[0][:2]) for y_i in y])\n",
    "\n",
    "y[0]=2\n",
    "x = StandardScaler().fit_transform(x)\n",
    "\n",
    "lda = LDA(n_components=2)\n",
    "lda_x = np.array(lda.fit(x, y).transform(x))\n",
    "\n",
    "\n",
    "importances = pd.DataFrame(data = abs(lda.scalings_), columns = ['PC1', 'PC2'])\n",
    "importances['Feature'] = features2\n",
    "display(importances.sort_values('PC1', ascending = False).reset_index(drop = True))\n",
    "imp = importances.sort_values('PC2', ascending = False)[0:30]\n",
    "ax1 = sns.barplot(x = 'PC2', y = 'Feature', data = imp)\n",
    "    \n",
    "fig = ax1.get_figure()\n",
    "fig.set_size_inches(4, 8)\n",
    "plt.rc('ytick', labelsize=10)\n",
    "ax1.set_xticklabels(ax1.get_xticklabels(), rotation = 90)  \n",
    "plt.show()\n",
    "imp2 = importances.sort_values('PC1', ascending = False)[0:30]\n",
    "ax2 = sns.barplot(x = 'PC1', y = 'Feature', data = imp2)\n",
    "fig = ax2.get_figure()\n",
    "fig.set_size_inches(4, 8)\n",
    "plt.rc('ytick', labelsize=10)\n",
    "ax2.set_xticklabels(ax2.get_xticklabels(), rotation = 90) \n",
    "plt.show()\n",
    "\n",
    "font = {'family' : 'Arial',\n",
    "        'size'   : 22}\n",
    "\n",
    "plt.rc('font', **font)\n",
    "\n",
    "scale=6\n",
    "target_names= ['S', 'HC']\n",
    "plt.figure()\n",
    "plt.figure(figsize = (8,8))\n",
    "for c, i, target_name in zip(\"rb\", [0, 1], target_names):\n",
    "    plt.scatter(lda_x[y == i,0] , lda_x[y == i,1] , c=c, label=target_name)\n",
    "    plt.legend(target_names*scale,fontsize=12)\n",
    "    plt.xlabel('LD1', fontsize = 20, family = 'Arial')\n",
    "    plt.ylabel('LD2', fontsize = 20, family = 'Arial')\n",
    "    plt.yticks(family = 'Arial', size = 10)\n",
    "    plt.xticks(family = 'Arial', size= 10)\n",
    "print(lda.explained_variance_ratio_)            \n",
    "plt.savefig(path + '\\\\' + 'mfn1-LDA.pdf', dpi=600 )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc59a2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MFN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32a5268",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = glob.glob('noam_may_analysis_fold1/MFN2/*.csv')\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    li.append(df)\n",
    "\n",
    "mfn2_df = pd.concat(li, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b131b719",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_types =  mfn2_df[\"Compound\"].values\n",
    "sns.histplot(cell_types)\n",
    "plt.show()\n",
    "print(f'Total number of cell types {len(cell_types)}')\n",
    "mfn2_df = mfn2_df.loc[cell_types != 'mdivi1 25uM']\n",
    "sns.histplot(mfn2_df[\"Compound\"].values)\n",
    "plt.show()\n",
    "print(f'Total number of cells {len(mfn2_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adfa2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_h = mfn2_df[(mfn2_df[\"Severity\"] == 'HC')] \n",
    "df_s = mfn2_df[(mfn2_df[\"Severity\"] == 'S')]\n",
    "frame_mfn2 = (df_h, df_s)\n",
    "frame_mfn2_f = pd.concat(frame_mfn2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cc74e4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MiguelW12\\AppData\\Local\\Temp\\ipykernel_9812\\3976612670.py:5: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  CLEAN_dfheatmapREAL_mfn2 = HEAT_DATA_mfn2.groupby(level=1).mean()\n"
     ]
    }
   ],
   "source": [
    "HEAT_DATA_mfn2 = frame_mfn2_f.copy()\n",
    "HEAT_DATA_mfn2[\"group_with_id\"] = HEAT_DATA_mfn2[\"Cell ID\"].astype(str) + HEAT_DATA_mfn2[\"Cell Type\"].astype(str) \n",
    "HEAT_DATA_mfn2.set_index([\"group_with_id\"], inplace = True,\n",
    "                            append = True, drop = False)\n",
    "CLEAN_dfheatmapREAL_mfn2 = HEAT_DATA_mfn2.groupby(level=1).mean()\n",
    "\n",
    "CLEAN_dfheatmap_final_mfn2 = CLEAN_dfheatmapREAL_mfn2.drop(columns=['Row', 'Plane', 'Timepoint', 'Number of Analyzed Fields', 'Height [µm]',\n",
    "                            'Time [s]', 'Column', \"Cell ID\"])\n",
    "CLEAN_dfheatmap_final_mfn2.fillna(0, inplace=True)\n",
    "CLEAN_dfheatmap_final_NORMAL_mfn2 = CLEAN_dfheatmap_final_mfn2.copy()\n",
    "CLEAN_dfheatmap_final_NORMAL_mfn2=(CLEAN_dfheatmap_final_NORMAL_mfn2-CLEAN_dfheatmap_final_NORMAL_mfn2.mean())/CLEAN_dfheatmap_final_NORMAL_mfn2.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "229c5558",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = 'mito_mfn2_cor.csv'\n",
    "CLEAN_dfheatmap_final_NORMAL_mfn2.to_csv(file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5d7cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_mfn2_f = frame_mfn2_f.drop(columns=['Row', 'Plane', 'Timepoint', 'Number of Analyzed Fields', 'Height [µm]',\n",
    "                            'Time [s]', 'Column', 'Compound', \"Cell Type\", \"Cell ID\"])\n",
    "\n",
    "frame_mfn2_f = frame_mfn2_f.rename(columns={\"Severity\":\"group\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e1c8d566",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_mfn2_f = frame_mfn2_f.dropna()\n",
    "summary_df = summarize_dataframe(frame_mfn2_f,'group')\n",
    "file1 = 'summary_mito_mfn2.csv'\n",
    "summary_df.to_csv(file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6aba2ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = 'mito_mfn2_combined.csv'\n",
    "frame_mfn2_f.to_csv(file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da731056",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA\n",
    "PCA_DATA = frame_mfn2_f.copy()\n",
    "PCA_DATA.fillna(0, inplace=True)\n",
    "features2 = PCA_DATA.columns[:54]  \n",
    "x2 = PCA_DATA.loc[:, features2].values\n",
    "y2 = PCA_DATA.loc[:,['group']].values\n",
    "x2 = StandardScaler().fit_transform(x2)\n",
    "pd.DataFrame(data = x2, columns = features2).head()\n",
    "pca2 = PCA(n_components=2)\n",
    "principalComponents2 = pca2.fit_transform(x2)\n",
    "PCA_DATA2_COMP = pd.DataFrame(data = principalComponents2\n",
    "             , columns = ['principal component 1', 'principal component 2'])\n",
    "importances2 = pd.DataFrame(data = abs(pca2.components_).transpose(), columns = ['PC1', 'PC2'])\n",
    "importances2['Feature'] = features2\n",
    "display(importances2.sort_values('PC1', ascending = False).reset_index(drop = True))\n",
    "imp2 = importances2.sort_values('PC2', ascending = False)[0:30]\n",
    "\n",
    "ax2 = sns.barplot(x = 'PC2', y = 'Feature', data = imp2)\n",
    "fig = ax2.get_figure()\n",
    "fig.set_size_inches(4, 8)\n",
    "ax2.set_xticklabels(ax2.get_xticklabels(), rotation = 90)  \n",
    "plt.show()\n",
    "    \n",
    "imp4 = importances2.sort_values('PC1', ascending = False)[0:30]\n",
    "ax2 = sns.barplot(x = 'PC1', y = 'Feature', data = imp4)\n",
    "\n",
    "fig = ax2.get_figure()\n",
    "fig.set_size_inches(4, 8)\n",
    "ax2.tick_params(axis='x', labelsize=6)\n",
    "ax2.tick_params(axis='y', labelsize=6)\n",
    "ax2.set_xticklabels(ax2.get_xticklabels(), rotation = 90)  \n",
    "plt.show()\n",
    "    \n",
    "PCA_DATA2_FINAL = pd.concat([PCA_DATA2_COMP, PCA_DATA[['group']]], axis = 1)\n",
    "PCA_FIG2 = plt.figure(figsize = (8,8))\n",
    "ax2 = PCA_FIG2.add_subplot(1,1,1) \n",
    "ax2.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax2.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax2.set_title('2 Component PCA', fontsize = 20)\n",
    "ax2.tick_params(axis='x', labelsize=6)\n",
    "ax2.tick_params(axis='y', labelsize=6)\n",
    "for tick in ax2.get_xticklabels():\n",
    "    tick.set_fontname(\"Arial\")\n",
    "for tick in ax2.get_yticklabels():\n",
    "    tick.set_fontname(\"Arial\")\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "targets2 = ['HC', 'S']\n",
    "colors2 = ['b', 'r']\n",
    "for target, color in zip(targets2,colors2):\n",
    "    indicesToKeep = PCA_DATA2_FINAL['group'] == target\n",
    "    ax2.scatter(PCA_DATA2_FINAL.loc[indicesToKeep, 'principal component 1']\n",
    "               , PCA_DATA2_FINAL.loc[indicesToKeep, 'principal component 2']\n",
    "               , c = color\n",
    "               , s = 50)\n",
    "    ax2.legend(targets2)\n",
    "    ax2.grid()\n",
    "\n",
    "    \n",
    "print(pca2.explained_variance_ratio_)\n",
    "plt.savefig(path + '//' + 'mfn2_PCA2.pdf', dpi = 300)\n",
    "plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3114382",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA\n",
    "PCA_DATA_CLEAN = frame_mfn2_f.copy()\n",
    "\n",
    "x = PCA_DATA_CLEAN.loc[:, features2].values\n",
    "y = PCA_DATA_CLEAN.loc[:,['group']].values\n",
    "y = np.array([int('HC' == y_i[0][:2]) for y_i in y])\n",
    "\n",
    "y[0]=2\n",
    "x = StandardScaler().fit_transform(x)\n",
    "\n",
    "lda = LDA(n_components=2)\n",
    "lda_x = np.array(lda.fit(x, y).transform(x))\n",
    "\n",
    "\n",
    "importances = pd.DataFrame(data = abs(lda.scalings_), columns = ['PC1', 'PC2'])\n",
    "importances['Feature'] = features2\n",
    "display(importances.sort_values('PC1', ascending = False).reset_index(drop = True))\n",
    "imp = importances.sort_values('PC2', ascending = False)[0:30]\n",
    "ax1 = sns.barplot(x = 'PC2', y = 'Feature', data = imp)\n",
    "    \n",
    "fig = ax1.get_figure()\n",
    "fig.set_size_inches(4, 8)\n",
    "plt.rc('ytick', labelsize=10)\n",
    "ax1.set_xticklabels(ax1.get_xticklabels(), rotation = 90)  \n",
    "plt.show()\n",
    "imp2 = importances.sort_values('PC1', ascending = False)[0:30]\n",
    "ax2 = sns.barplot(x = 'PC1', y = 'Feature', data = imp2)\n",
    "fig = ax2.get_figure()\n",
    "fig.set_size_inches(4, 8)\n",
    "plt.rc('ytick', labelsize=10)\n",
    "ax2.set_xticklabels(ax2.get_xticklabels(), rotation = 90) \n",
    "plt.show()\n",
    "\n",
    "font = {'family' : 'Arial',\n",
    "        'size'   : 22}\n",
    "\n",
    "plt.rc('font', **font)\n",
    "\n",
    "scale=6\n",
    "target_names= ['S', 'HC']\n",
    "plt.figure()\n",
    "plt.figure(figsize = (8,8))\n",
    "for c, i, target_name in zip(\"rb\", [0, 1], target_names):\n",
    "    plt.scatter(lda_x[y == i,0] , lda_x[y == i,1] , c=c, label=target_name)\n",
    "    plt.legend(target_names*scale,fontsize=12)\n",
    "    plt.xlabel('LD1', fontsize = 20, family = 'Arial')\n",
    "    plt.ylabel('LD2', fontsize = 20, family = 'Arial')\n",
    "    plt.yticks(family = 'Arial', size = 10)\n",
    "    plt.xticks(family = 'Arial', size= 10)\n",
    "print(lda.explained_variance_ratio_)            \n",
    "plt.savefig(path + '\\\\' + 'mfn2-LDA.pdf', dpi=600 )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c902f2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "###prepare data for batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd63f703",
   "metadata": {},
   "outputs": [],
   "source": [
    "##drp\n",
    "input_path = 'noam_may_analysis_fold1/DRP1'\n",
    "output_path = 'noam_may_analysis_fold1/DRP1/ready_data_drp1'\n",
    "add_exp_column_to_csv_files(input_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef8104c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##vat\n",
    "input_path = 'noam_may_analysis_fold1/VAT1'\n",
    "output_path = 'noam_may_analysis_fold1/VAT1/ready_data_VAT1'\n",
    "add_exp_column_to_csv_files(input_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475c0786",
   "metadata": {},
   "outputs": [],
   "source": [
    "##mfn1\n",
    "input_path = 'noam_may_analysis_fold1/MFN1'\n",
    "output_path = 'noam_may_analysis_fold1/MFN1/ready_data_MFN1'\n",
    "add_exp_column_to_csv_files(input_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2740c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "##mfn2\n",
    "input_path = 'noam_may_analysis_fold1/MFN2'\n",
    "output_path = 'noam_may_analysis_fold1/MFN2/ready_data_MFN2'\n",
    "add_exp_column_to_csv_files(input_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f8f19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_plate_column('C:/Users/Uri8s/DRP1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1b96a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_plate_column('C:/Users/MiguelW12/PycharmProjects/noam_may_analysis_fold1/VAT1/ready_data_VAT1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279208b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_plate_column('C:/Users/MiguelW12/PycharmProjects/noam_may_analysis_fold1/MFN1/ready_data_MFN1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7720e3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_plate_column('C:/Users/MiguelW12/PycharmProjects/noam_may_analysis_fold1/MFN2/ready_data_MFN2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d207c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = ('Results_mito_markers_ready_with_plate')\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "    print('Output folder created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8051f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drp1\n",
    "all_files = glob.glob('C:/Users/Uri8s/output_drp/*.csv')\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    li.append(df)\n",
    "\n",
    "drp1_df = pd.concat(li, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0530115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "drp1_df = drp1_df.drop(columns=[\"Timepoint\", \"Time [s]\", \"Plane\", \"Row\", \"Column\", \"Number of Analyzed Fields\", \"Height [µm]\"\n",
    "                               ])\n",
    "drp1_df[\"sample\"] = drp1_df[\"Severity\"].astype(str) + drp1_df[\"Cell ID\"].astype(str) + drp1_df[\"Compound\"].astype(str)  \n",
    "#selected feat\n",
    "drp1_df = drp1_df[[\"spots_chanel_4_final - Corrected Spot Intensity - Mean per Well\",\n",
    "\"spots_chanel_4_final - Intensity_Spot chanel_4_final Mean - Mean per Well\",\n",
    "\"all_cells - chanel_4_Intensity Mean - Mean per Well\", \"total_ir_chanel_4_normalized\",\n",
    "\"modified_spots_chanel_4 - Total Spot Area - Mean per Well\",\n",
    "\"all_cells - chanel_4 Profile 2/5  - Mean per Well\", \"all_cells - chanel_4 SER Spot 1 px - Mean per Well\",\n",
    "\"all_cells - chanel_4 Gabor Max 2 px w2 - Mean per Well\",\n",
    "\"all_cells - chanel_4 SER Edge 1 px - Mean per Well\", \"sample\", \"Severity\", \"Cell ID\", \"Compound\",\n",
    "                  \"all_cells - chanel_1 Ratio Width to Length - Mean per Well\",\n",
    "                  \"all_cells - chanel_1 Roundness - Mean per Well\", \"plate\"]]\n",
    "#rename features\n",
    "drp1_df = drp1_df.rename(\n",
    "     columns={ \"spots_chanel_4_final - Corrected Spot Intensity - Mean per Well\":\"intensity_1\",\n",
    "\"spots_chanel_4_final - Intensity_Spot chanel_4_final Mean - Mean per Well\":\"intensity_2\",\n",
    "\"all_cells - chanel_4_Intensity Mean - Mean per Well\":\"intensity_3\",\n",
    "\n",
    "\"total_ir_chanel_4_normalized\":\"area_1\",\n",
    "\"modified_spots_chanel_4 - Total Spot Area - Mean per Well\": \"area_2\",\n",
    "\"all_cells - chanel_4 Profile 2/5  - Mean per Well\": \"area_3\",\n",
    "\n",
    "\"all_cells - chanel_4 SER Spot 1 px - Mean per Well\": \"texture_1\",\n",
    "\"all_cells - chanel_4 Gabor Max 2 px w2 - Mean per Well\": \"texture_2\",\n",
    "\"all_cells - chanel_4 SER Edge 1 px - Mean per Well\": \"texture_3\" \n",
    "             })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9688fbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "drp1_data_for_batch = drp1_df.groupby(['sample', 'EXP', \"Severity\", \"Cell ID\", \"Compound\", \"plate\"],as_index=False).agg({\n",
    "                                                    'intensity_1':'mean',\n",
    "                                                    'intensity_2':'mean',\n",
    "                                                    'intensity_3':'mean',\n",
    "                                                    'area_1':'mean', \n",
    "                                                    'area_2':'mean',\n",
    "                                                    'area_3':'mean', \n",
    "                                                    'texture_1':'mean',\n",
    "                                                    'texture_2':'mean', \n",
    "                                                    'texture_3':'mean', \n",
    "                                                                })\n",
    "drp1_data_for_batch.columns = [str(col) + '_drp1' for col in drp1_data_for_batch.columns]\n",
    "drp1_data_for_batch = drp1_data_for_batch.rename(\n",
    "     columns={\"sample_drp1\":\"sample\",\n",
    "              \"EXP_drp1\":\"EXP\",\n",
    "              \"plate_drp1\":\"plate\",\n",
    "              \"Severity_drp1\":\"severity\",\n",
    "              \"Cell ID_drp1\":\"cell_id\",\n",
    "              \"Compound_drp1\":\"compound\"\n",
    "             })\n",
    "print(drp1_data_for_batch)\n",
    "drp1_data_for_batch.to_csv(path + \"//\" + 'drp1.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6641b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mfn1\n",
    "all_files = glob.glob('noam_may_analysis_fold1/MFN1/ready_data_mfn1/output/*.csv')\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    li.append(df)\n",
    "\n",
    "mfn1_df = pd.concat(li, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d84876",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfn1_df = mfn1_df.drop(columns=[\"Timepoint\", \"Time [s]\", \"Plane\", \"Row\", \"Column\", \"Number of Analyzed Fields\", \"Height [µm]\"\n",
    "                               ])\n",
    "mfn1_df[\"sample\"] = mfn1_df[\"Severity\"].astype(str) + mfn1_df[\"Cell ID\"].astype(str) + mfn1_df[\"Compound\"].astype(str)  \n",
    "\n",
    "mfn1_df = mfn1_df[[\"spots_chanel_4_final - Corrected Spot Intensity - Mean per Well\",\n",
    "\"spots_chanel_4_final - Intensity_Spot chanel_4_final Mean - Mean per Well\",\n",
    "\"all_cells - chanel_4_Intensity Mean - Mean per Well\", \"total_ir_chanel_4_normalized\",\n",
    "\"modified_spots_chanel_4 - Total Spot Area - Mean per Well\",\n",
    "\"all_cells - chanel_4 Profile 2/5  - Mean per Well\", \"all_cells - chanel_4 SER Spot 1 px - Mean per Well\",\n",
    "\"all_cells - chanel_4 Gabor Max 2 px w2 - Mean per Well\",\n",
    "\"all_cells - chanel_4 SER Edge 1 px - Mean per Well\", \"sample\", \"EXP\", \"Severity\", \"Cell ID\", \"Compound\", \"plate\"]]\n",
    "#rename features\n",
    "mfn1_df = mfn1_df.rename(\n",
    "     columns={ \"spots_chanel_4_final - Corrected Spot Intensity - Mean per Well\":\"intensity_1\",\n",
    "\"spots_chanel_4_final - Intensity_Spot chanel_4_final Mean - Mean per Well\":\"intensity_2\",\n",
    "\"all_cells - chanel_4_Intensity Mean - Mean per Well\":\"intensity_3\",\n",
    "\n",
    "\"total_ir_chanel_4_normalized\":\"area_1\",\n",
    "\"modified_spots_chanel_4 - Total Spot Area - Mean per Well\": \"area_2\",\n",
    "\"all_cells - chanel_4 Profile 2/5  - Mean per Well\": \"area_3\",\n",
    "\n",
    "\"all_cells - chanel_4 SER Spot 1 px - Mean per Well\": \"texture_1\",\n",
    "\"all_cells - chanel_4 Gabor Max 2 px w2 - Mean per Well\": \"texture_2\",\n",
    "\"all_cells - chanel_4 SER Edge 1 px - Mean per Well\": \"texture_3\" \n",
    "             })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5929ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfn1_data_for_batch = mfn1_df.groupby(['sample', 'EXP', \"Severity\", \"Cell ID\", \"Compound\", \"plate\"],as_index=False).agg({\n",
    "                                                    'intensity_1':'mean',\n",
    "                                                    'intensity_2':'mean',\n",
    "                                                    'intensity_3':'mean',\n",
    "                                                    'area_1':'mean', \n",
    "                                                    'area_2':'mean',\n",
    "                                                    'area_3':'mean', \n",
    "                                                    'texture_1':'mean',\n",
    "                                                    'texture_2':'mean', \n",
    "                                                    'texture_3':'mean', \n",
    "                                                                })\n",
    "mfn1_data_for_batch.columns = [str(col) + '_mfn1' for col in mfn1_data_for_batch.columns]\n",
    "mfn1_data_for_batch = mfn1_data_for_batch.rename(\n",
    "     columns={\"sample_mfn1\":\"sample\",\n",
    "              \"EXP_mfn1\":\"EXP\",\n",
    "              \"plate_mfn1\":\"plate\",\n",
    "              \"Severity_mfn1\":\"severity\",\n",
    "              \"Cell ID_mfn1\":\"cell_id\",\n",
    "              \"Compound_mfn1\":\"compound\"\n",
    "             })\n",
    "print(mfn1_data_for_batch)\n",
    "mfn1_data_for_batch.to_csv(path + \"//\" + 'mfn1.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27d610f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mfn2\n",
    "all_files = glob.glob('noam_may_analysis_fold1/MFN2/ready_data_mfn2/output/*.csv')\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    li.append(df)\n",
    "\n",
    "mfn2_df = pd.concat(li, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5318cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfn2_df = mfn2_df.drop(columns=[\"Timepoint\", \"Time [s]\", \"Plane\", \"Row\", \"Column\", \"Number of Analyzed Fields\", \"Height [µm]\"\n",
    "                               ])\n",
    "mfn2_df[\"sample\"] = mfn2_df[\"Severity\"].astype(str) + mfn2_df[\"Cell ID\"].astype(str) + mfn2_df[\"Compound\"].astype(str)  \n",
    "\n",
    "mfn2_df = mfn2_df[[\"spots_chanel_4_final - Corrected Spot Intensity - Mean per Well\",\n",
    "\"spots_chanel_4_final - Intensity_Spot chanel_4_final Mean - Mean per Well\",\n",
    "\"all_cells - chanel_4_Intensity Mean - Mean per Well\", \"total_ir_chanel_4_normalized\",\n",
    "\"modified_spots_chanel_4 - Total Spot Area - Mean per Well\",\n",
    "\"all_cells - chanel_4 Profile 2/5  - Mean per Well\", \"all_cells - chanel_4 SER Spot 1 px - Mean per Well\",\n",
    "\"all_cells - chanel_4 Gabor Max 2 px w2 - Mean per Well\",\n",
    "\"all_cells - chanel_4 SER Edge 1 px - Mean per Well\", \"sample\", \"EXP\", \"Severity\", \"Cell ID\", \"Compound\", \"plate\"]]\n",
    "#rename features\n",
    "mfn2_df = mfn2_df.rename(\n",
    "     columns={ \"spots_chanel_4_final - Corrected Spot Intensity - Mean per Well\":\"intensity_1\",\n",
    "\"spots_chanel_4_final - Intensity_Spot chanel_4_final Mean - Mean per Well\":\"intensity_2\",\n",
    "\"all_cells - chanel_4_Intensity Mean - Mean per Well\":\"intensity_3\",\n",
    "\n",
    "\"total_ir_chanel_4_normalized\":\"area_1\",\n",
    "\"modified_spots_chanel_4 - Total Spot Area - Mean per Well\": \"area_2\",\n",
    "\"all_cells - chanel_4 Profile 2/5  - Mean per Well\": \"area_3\",\n",
    "\n",
    "\"all_cells - chanel_4 SER Spot 1 px - Mean per Well\": \"texture_1\",\n",
    "\"all_cells - chanel_4 Gabor Max 2 px w2 - Mean per Well\": \"texture_2\",\n",
    "\"all_cells - chanel_4 SER Edge 1 px - Mean per Well\": \"texture_3\" \n",
    "             })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cdecb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfn2_data_for_batch = mfn2_df.groupby(['sample', 'EXP', \"Severity\", \"Cell ID\", \"Compound\", \"plate\"],as_index=False).agg({\n",
    "                                                    'intensity_1':'mean',\n",
    "                                                    'intensity_2':'mean',\n",
    "                                                    'intensity_3':'mean',\n",
    "                                                    'area_1':'mean', \n",
    "                                                    'area_2':'mean',\n",
    "                                                    'area_3':'mean', \n",
    "                                                    'texture_1':'mean',\n",
    "                                                    'texture_2':'mean', \n",
    "                                                    'texture_3':'mean', \n",
    "                                                                })\n",
    "mfn2_data_for_batch.columns = [str(col) + '_mfn2' for col in mfn2_data_for_batch.columns]\n",
    "mfn2_data_for_batch = mfn2_data_for_batch.rename(\n",
    "     columns={\"sample_mfn2\":\"sample\",\n",
    "              \"EXP_mfn2\":\"EXP\",\n",
    "              \"plate_mfn2\":\"plate\",\n",
    "              \"Severity_mfn2\":\"severity\",\n",
    "              \"Cell ID_mfn2\":\"cell_id\",\n",
    "              \"Compound_mfn2\":\"compound\"\n",
    "             })\n",
    "print(mfn2_data_for_batch)\n",
    "mfn2_data_for_batch.to_csv(path + \"//\" + 'mfn2.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049c758c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vat\n",
    "all_files = glob.glob('noam_may_analysis_fold1/VAT1/ready_data_vat1/output/*.csv')\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    li.append(df)\n",
    "\n",
    "vat_df = pd.concat(li, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95cb24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vat_df = vat_df.drop(columns=[\"Timepoint\", \"Time [s]\", \"Plane\", \"Row\", \"Column\", \"Number of Analyzed Fields\", \"Height [µm]\"\n",
    "                               ])\n",
    "vat_df[\"sample\"] = vat_df[\"Severity\"].astype(str) + vat_df[\"Cell ID\"].astype(str) + vat_df[\"Compound\"].astype(str)  \n",
    "\n",
    "vat_df = vat_df[[\"spots_chanel_4_final - Corrected Spot Intensity - Mean per Well\",\n",
    "\"spots_chanel_4_final - Intensity_Spot chanel_4_final Mean - Mean per Well\",\n",
    "\"all_cells - chanel_4_Intensity Mean - Mean per Well\", \"total_ir_chanel_4_normalized\",\n",
    "\"modified_spots_chanel_4 - Total Spot Area - Mean per Well\",\n",
    "\"all_cells - chanel_4 Profile 2/5  - Mean per Well\", \"all_cells - chanel_4 SER Spot 1 px - Mean per Well\",\n",
    "\"all_cells - chanel_4 Gabor Max 2 px w2 - Mean per Well\",\n",
    "\"all_cells - chanel_4 SER Edge 1 px - Mean per Well\", \"sample\", \"EXP\", \"Severity\", \"Cell ID\", \"Compound\", \"plate\"]]\n",
    "#rename features\n",
    "vat_df = vat_df.rename(\n",
    "     columns={ \"spots_chanel_4_final - Corrected Spot Intensity - Mean per Well\":\"intensity_1\",\n",
    "\"spots_chanel_4_final - Intensity_Spot chanel_4_final Mean - Mean per Well\":\"intensity_2\",\n",
    "\"all_cells - chanel_4_Intensity Mean - Mean per Well\":\"intensity_3\",\n",
    "\n",
    "\"total_ir_chanel_4_normalized\":\"area_1\",\n",
    "\"modified_spots_chanel_4 - Total Spot Area - Mean per Well\": \"area_2\",\n",
    "\"all_cells - chanel_4 Profile 2/5  - Mean per Well\": \"area_3\",\n",
    "\n",
    "\"all_cells - chanel_4 SER Spot 1 px - Mean per Well\": \"texture_1\",\n",
    "\"all_cells - chanel_4 Gabor Max 2 px w2 - Mean per Well\": \"texture_2\",\n",
    "\"all_cells - chanel_4 SER Edge 1 px - Mean per Well\": \"texture_3\" \n",
    "             })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e844a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vat_data_for_batch = vat_df.groupby(['sample', 'EXP', \"Severity\", \"Cell ID\", \"Compound\", \"plate\"],as_index=False).agg({\n",
    "                                                    'intensity_1':'mean',\n",
    "                                                    'intensity_2':'mean',\n",
    "                                                    'intensity_3':'mean',\n",
    "                                                    'area_1':'mean', \n",
    "                                                    'area_2':'mean',\n",
    "                                                    'area_3':'mean', \n",
    "                                                    'texture_1':'mean',\n",
    "                                                    'texture_2':'mean', \n",
    "                                                    'texture_3':'mean', \n",
    "                                                                })\n",
    "vat_data_for_batch.columns = [str(col) + '_vat' for col in vat_data_for_batch.columns]\n",
    "vat_data_for_batch = vat_data_for_batch.rename(\n",
    "     columns={\"sample_vat\":\"sample\",\n",
    "              \"EXP_vat\":\"EXP\",\n",
    "              \"plate_vat\":\"plate\",\n",
    "              \"Severity_vat\":\"severity\",\n",
    "              \"Cell ID_vat\":\"cell_id\",\n",
    "              \"Compound_vat\":\"compound\"\n",
    "             })\n",
    "print(vat_data_for_batch)\n",
    "vat_data_for_batch.to_csv(path + \"//\" + 'vat.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b9dff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### marker downstream analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c72a1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## markers integ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df754f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##radar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9cdfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_integ = pd.read_csv('all_markers_df_integ.csv')\n",
    "df_integ.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aceeb15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# condition mapping for new column\n",
    "condition_mapping = {'M': 'Mild', 'S': 'Severe', 'P': 'Premanifest', 'H': 'Control'}\n",
    "\n",
    "#  create the new column\n",
    "df_integ = create_new_column_based_on_first_letter(df_integ, 'group_with_id', 'group', condition_mapping)\n",
    "df_integ[\"index2\"] = df_integ[\"DS\"].astype(str) + df_integ[\"group_with_id\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eba010b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_value_to_remove = 'mdivi1 25uM'\n",
    "df_integ = remove_rows_containing_specific_value(df_integ, 'group_with_id', specific_value_to_remove)\n",
    "df_integ = df_integ.groupby(\"group\").mean()\n",
    "df_integ = df_integ.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b935a7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_integ.set_index([\"group\"], inplace = True,\n",
    "                            append = True, drop = False)\n",
    "df_integ = df_integ.drop(columns=[\"group\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7478505",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale\n",
    "min_max = df_integ.copy()\n",
    "min_max = min_max.reset_index()\n",
    "numeric_columns = min_max.columns[min_max.dtypes == 'float64']\n",
    "group_column = 'group'\n",
    "\n",
    "# min-max Scaling\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = min_max.copy()\n",
    "df_scaled[numeric_columns] = scaler.fit_transform(min_max[numeric_columns])\n",
    "df_scaled.set_index([\"group\"], inplace = True,\n",
    "                            append = True, drop = False)\n",
    "df_scaled = df_scaled.drop(columns=[\"group\"])\n",
    "df_scaled = df_scaled + 1\n",
    "df_scaled = df_scaled.drop(columns=[\"level_0\"])\n",
    "df_scaled = df_scaled.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2424c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def radar_plot(df, features, group_column, color_mapping=None):\n",
    "\n",
    "\n",
    "    missing_features = set(features) - set(df.columns)\n",
    "    if missing_features:\n",
    "        raise ValueError(f\"The following features are not present in the DataFrame: {missing_features}\")\n",
    "\n",
    "    # columns\n",
    "    selected_columns = [group_column] + features\n",
    "    df_selected = df[selected_columns]\n",
    "\n",
    "\n",
    "    if color_mapping:\n",
    "        color_discrete_map = color_mapping\n",
    "    else:\n",
    "        color_discrete_map = None\n",
    "\n",
    "\n",
    "    dash_styles = ['solid', 'dash', 'dot', 'dashdot']\n",
    "\n",
    "\n",
    "    fig = go.Figure()\n",
    "    for i, group in enumerate(df[group_column].unique()):\n",
    "        group_data = df[df[group_column] == group]\n",
    "        fig.add_trace(go.Scatterpolar(\n",
    "            r=group_data[features].mean(),\n",
    "            theta=features,\n",
    "            mode='lines+markers',  \n",
    "            name=f'{group} - Mean',\n",
    "            line_shape='linear',\n",
    "            fill='toself',  \n",
    "            line=dict(color=color_discrete_map[group] if color_discrete_map else None, dash=dash_styles[i % len(dash_styles)]),\n",
    "            marker=dict(color=color_discrete_map[group] if color_discrete_map else None, size=10, symbol='circle'),\n",
    "        ))\n",
    "\n",
    "\n",
    "    fig.update_layout(\n",
    "        polar=dict(radialaxis=dict(visible=True, range=[0, df[features].max().max()], tickfont=dict(size=12))),\n",
    "        showlegend=True,\n",
    "        legend=dict(x=0, y=1), \n",
    "        template='plotly', \n",
    "       \n",
    "    )\n",
    "\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "    #  features and group column\n",
    "    features = ['drp1_intensity','mfn1_intensity','mfn2_intensity',\n",
    "              'vat1_intensity']\n",
    "    group_column = 'group'\n",
    "\n",
    "    # colors \n",
    "    color_mapping = {'Control': 'blue', 'Severe': 'red',\n",
    "                    'Mild': 'green', 'Premanifest':'yellow'}\n",
    "\n",
    "    # run function\n",
    "    radar_plot(df_scaled, features, group_column, color_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70c8897",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
