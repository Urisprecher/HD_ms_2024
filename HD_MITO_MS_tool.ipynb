{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90479b56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MiguelW12\\PycharmProjects\\outdated\\utils.py:14: OutdatedPackageWarning: The package pingouin is out of date. Your version is 0.5.2, the latest is 0.5.3.\n",
      "Set the environment variable OUTDATED_IGNORE=1 to disable these warnings.\n",
      "  return warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stat\n",
    "import scipy\n",
    "from scipy.stats import pearsonr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix \n",
    "from psynlig import plot_correlation_heatmap\n",
    "import pingouin as pg\n",
    "from scipy.stats import pearsonr\n",
    "import plotly.express as px\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import kaleido\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix \n",
    "%matplotlib inline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.manifold import TSNE\n",
    "from colour import Color\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "import sklearn.linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import svm\n",
    "from sklearn.datasets import make_moons, make_blobs\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix \n",
    "from psynlig import plot_correlation_heatmap\n",
    "from scipy.stats import pearsonr\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "from plotly.tools import FigureFactory as FF\n",
    "import chart_studio as cs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import chart_studio.plotly as py\n",
    "from plotly.offline import iplot\n",
    "import pandas as pd\n",
    "import numpy as np  \n",
    "%matplotlib inline  \n",
    "from plotly import __version__ \n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import cufflinks as cf\n",
    "init_notebook_mode(connected=True)\n",
    "cf.go_offline()\n",
    "import chart_studio.plotly as py\n",
    "from sklearn import preprocessing\n",
    "from pandas.plotting import scatter_matrix \n",
    "from psynlig import plot_correlation_heatmap\n",
    "from scipy.stats import pearsonr\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix \n",
    "%matplotlib inline\n",
    "from pptx import Presentation \n",
    "from pptx.util import Inches \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.manifold import TSNE\n",
    "from colour import Color\n",
    "import matplotlib as mpl\n",
    "import scipy, random\n",
    "from matplotlib.patches import Ellipse\n",
    "import matplotlib.transforms as transforms\n",
    "from pandas.api.types import CategoricalDtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f143786",
   "metadata": {},
   "outputs": [],
   "source": [
    "##functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e91cf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_dataframe(df, group_col):\n",
    "\n",
    "    grouped_df = df.groupby(group_col)\n",
    "   \n",
    "    agg_dict = {}\n",
    "   \n",
    "    for col in df.columns:\n",
    "        if col != group_col:\n",
    "            agg_dict[col] = [\n",
    "                ('count', 'count'),\n",
    "                ('mean', 'mean'),\n",
    "                ('std', 'std'),\n",
    "                ('min', 'min'),\n",
    "                ('25%', lambda x: np.quantile(x, 0.25)),\n",
    "                ('median', 'median'),\n",
    "                ('75%', lambda x: np.quantile(x, 0.75)),\n",
    "                ('max', 'max')\n",
    "                \n",
    "            ]\n",
    "   \n",
    "    summary_df = grouped_df.agg(agg_dict)\n",
    "   \n",
    "    summary_df.columns = [f'{col}_{stat}' for col, stat in summary_df.columns]\n",
    "   \n",
    "    summary_df.index.name = group_col\n",
    "   \n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dd2b12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_check(df):\n",
    "    \n",
    "        for col in df:\n",
    "            plt.hist(df[col])\n",
    "            plt.title(f\"Histogram of ({col})\")\n",
    "            plt.xlabel(f\"log({col})\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.show()\n",
    "            plt.savefig(path + '//' + f\"hist{col}.pdf\", dpi = 600)\n",
    "            plt.close()\n",
    "            print(\"hist_done!\")\n",
    "            plt.figure()\n",
    "            scipy.stats.probplot(df[col], dist=\"norm\", plot=plt)\n",
    "            plt.title(f\"Q-Q-{col}\")\n",
    "            plt.show\n",
    "            plt.savefig(path + '//' + f\"Q-Q{col}.pdf\", dpi = 600)\n",
    "            plt.close()\n",
    "            print(\"qq_done:)\")\n",
    "            plt.figure()\n",
    "            sns.distplot(df[col], kde = True, color ='red', bins = 30)\n",
    "            plt.title(f\"dist-{col}\")\n",
    "            plt.show\n",
    "            plt.savefig(path + '//' + f\"dist{col}.pdf\", dpi = 600)\n",
    "            plt.close()\n",
    "            print(\"on it:)\")\n",
    "            print(\"done:)\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6ae3978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scores(df, output_file1):\n",
    "    up_scores = []\n",
    "    low_scores = []\n",
    "    above_count = []\n",
    "    below_count = []\n",
    "    for col in df.columns:\n",
    "        up_score = df[col].mean() + 3 * df[col].std()\n",
    "        low_score = df[col].mean() - 3 * df[col].std()\n",
    "        above = df[df[col] > up_score]\n",
    "        below = df[df[col] < low_score]\n",
    "        above_count.append(len(above))\n",
    "        below_count.append(len(below))\n",
    "        df = df[(df[col] >= low_score) & (df[col] <= up_score)]\n",
    "        up_scores.append(up_score)\n",
    "        low_scores.append(low_score)\n",
    "    output = pd.DataFrame({'Column': df.columns, 'Up Score': up_scores, 'Low Score': low_scores, 'Above Count': above_count, 'Below Count': below_count})\n",
    "    output.to_csv(output_file1, index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af4b7daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iqr(df, output_file2):\n",
    "    up_bounds = []\n",
    "    low_bounds = []\n",
    "    above_count = []\n",
    "    below_count = []\n",
    "    for col in df.columns:\n",
    "        percentile25th = df[col].quantile(0.25)\n",
    "        percentile75th = df[col].quantile(0.75)\n",
    "        iqr = percentile75th - percentile25th\n",
    "        up_bound = percentile75th + 1.5 * iqr\n",
    "        low_bound = percentile25th - 1.5 * iqr\n",
    "        above = df[df[col] > up_bound]\n",
    "        below = df[df[col] < low_bound]\n",
    "        above_count.append(len(above))\n",
    "        below_count.append(len(below))\n",
    "        df = df[(df[col] >= low_bound) & (df[col] <= up_bound)]\n",
    "        up_bounds.append(up_bound)\n",
    "        low_bounds.append(low_bound)\n",
    "    output = pd.DataFrame({'Column': df.columns, 'Up Bound': up_bounds, 'Low Bound': low_bounds, 'Above Count': above_count, 'Below Count': below_count})\n",
    "    output.to_csv(output_file2, index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9fd79e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_percentiles(df, output_file3):\n",
    "    up_bounds = []\n",
    "    low_bounds = []\n",
    "    above_count = []\n",
    "    below_count = []\n",
    "    for col in df.columns:\n",
    "        percentile_low = df[col].quantile(0.01)\n",
    "        percentile_high = df[col].quantile(0.99)\n",
    "        up_bound = percentile_high \n",
    "        low_bound = percentile_low\n",
    "        above = df[df[col] > percentile_high]\n",
    "        below = df[df[col] < percentile_low]\n",
    "        above_count.append(len(above))\n",
    "        below_count.append(len(below))\n",
    "        df = df[(df[col] >= percentile_low) & (df[col] <= percentile_high)]\n",
    "        up_bounds.append(up_bound)\n",
    "        low_bounds.append(low_bound)\n",
    "    output = pd.DataFrame({'Column': df.columns, 'Up Bound': up_bounds, 'Low Bound': low_bounds, 'Above Count': above_count, 'Below Count': below_count})\n",
    "    output.to_csv(output_file3, index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41ea67b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_check_if_pre(frame):\n",
    "    \n",
    "    print(\"frame_ready\")\n",
    "    for col in frame:\n",
    "            \n",
    "            plt.hist(frame[col])\n",
    "            plt.title(f\"Histogram of log({col})\")\n",
    "            plt.xlabel(f\"log({col})\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.show()\n",
    "            plt.savefig(curr_out_path + '//' + f\"hist_pre{col}.pdf\", dpi = 600)\n",
    "            plt.close()\n",
    "            print(\"hist_done!\")\n",
    "            plt.figure()\n",
    "            scipy.stats.probplot(frame[col], dist=\"norm\", plot=plt)\n",
    "            plt.title(f\"Q-Q-{col}\")\n",
    "            plt.show\n",
    "            plt.savefig(curr_out_path + '//' + f\"Q-Q_pre{col}.pdf\", dpi = 600)\n",
    "            plt.close()\n",
    "            print(\"qq_done:)\")\n",
    "            plt.figure()\n",
    "            sns.distplot(frame[col], kde = True, color ='red', bins = 30)\n",
    "            plt.title(f\"dist-{col}\")\n",
    "            plt.show\n",
    "            plt.savefig(curr_out_path + '//' + f\"dist_pre{col}.pdf\", dpi = 600)\n",
    "            plt.close()\n",
    "            print(\"on it:)\")\n",
    "            print(\"done:)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "001e0737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_check_if_hc(frame):\n",
    "    frame = frame.drop(columns=['Local Outlier Factor 30', 'Local Outlier Factor 30_outliers',\n",
    "       'Local Outlier Factor 5', 'Local Outlier Factor 5_outliers',\n",
    "       'Local Outlier Factor 8', 'Local Outlier Factor 8_outliers', 'group',\n",
    "       'group_with_id', 'group_with_pc'])\n",
    "    print(\"frame_ready\")\n",
    "    for col in frame:\n",
    "            \n",
    "            plt.hist(frame[col])\n",
    "            plt.title(f\"Histogram of hc({col})\")\n",
    "            plt.xlabel(f\"hc({col})\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.show()\n",
    "            plt.savefig(path + '//' + f\"hist_hc{col}.pdf\", dpi = 600)\n",
    "            plt.close()\n",
    "            print(\"hist_done!\")\n",
    "            plt.figure()\n",
    "            scipy.stats.probplot(frame[col], dist=\"norm\", plot=plt)\n",
    "            plt.title(f\"Q-Q-{col}\")\n",
    "            plt.show\n",
    "            plt.savefig(path + '//' + f\"Q-Q_hc{col}.pdf\", dpi = 600)\n",
    "            plt.close()\n",
    "            print(\"qq_done:)\")\n",
    "            plt.figure()\n",
    "            sns.distplot(frame[col], kde = True, color ='red', bins = 30)\n",
    "            plt.title(f\"dist-{col}\")\n",
    "            plt.show\n",
    "            plt.savefig(path + '//' + f\"dist_hc{col}.pdf\", dpi = 600)\n",
    "            plt.close()\n",
    "            print(\"on it:)\")\n",
    "            print(\"done:)\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "94622fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_check_if_max(frame):\n",
    "    \n",
    "    print(\"frame_ready\")\n",
    "    for col in frame:\n",
    "            plt.hist(frame[col])\n",
    "            plt.title(f\"Histogram of max({col})\")\n",
    "            plt.xlabel(f\"max({col})\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.show()\n",
    "            plt.savefig(path + '//' + f\"hist_max{col}.pdf\", dpi = 600)\n",
    "            plt.close()\n",
    "            print(\"hist_done!\")\n",
    "            plt.figure()\n",
    "            scipy.stats.probplot(frame[col], dist=\"norm\", plot=plt)\n",
    "            plt.title(f\"Q-Q-{col}\")\n",
    "            plt.show\n",
    "            plt.savefig(path + '//' + f\"Q-Q_max{col}.pdf\", dpi = 600)\n",
    "            plt.close()\n",
    "            print(\"qq_done:)\")\n",
    "            plt.figure()\n",
    "            sns.distplot(frame[col], kde = True, color ='red', bins = 30)\n",
    "            plt.title(f\"dist-{col}\")\n",
    "            plt.show\n",
    "            plt.savefig(path + '//' + f\"dist_max{col}.pdf\", dpi = 600)\n",
    "            plt.close()\n",
    "            print(\"on it:)\")\n",
    "            print(\"done:)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "86be8dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_check_if_log(frame):\n",
    "    \n",
    "    print(\"frame_ready\")\n",
    "    for col in frame:\n",
    "            plt.hist(frame[col])\n",
    "            plt.title(f\"Histogram of log({col})\")\n",
    "            plt.xlabel(f\"log({col})\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.show()\n",
    "            plt.savefig(path + '//' + f\"hist_log{col}.pdf\", dpi = 600)\n",
    "            plt.close()\n",
    "            print(\"hist_done!\")\n",
    "            plt.figure()\n",
    "            scipy.stats.probplot(frame[col], dist=\"norm\", plot=plt)\n",
    "            plt.title(f\"Q-Q-{col}\")\n",
    "            plt.show\n",
    "            plt.savefig(path + '//' + f\"Q-Q_log{col}.pdf\", dpi = 600)\n",
    "            plt.close()\n",
    "            print(\"qq_done:)\")\n",
    "            plt.figure()\n",
    "            sns.distplot(frame[col], kde = True, color ='red', bins = 30)\n",
    "            plt.title(f\"dist-{col}\")\n",
    "            plt.show\n",
    "            plt.savefig(path + '//' + f\"dist_log{col}.pdf\", dpi = 600)\n",
    "            plt.close()\n",
    "            print(\"on it:)\")\n",
    "            print(\"done:)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ce4f8fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_check_if_log_2(frame):\n",
    "    \n",
    "    print(\"frame_ready\")\n",
    "    for col in frame:\n",
    "            plt.hist(frame[col])\n",
    "            plt.title(f\"Histogram of log+1({col})\")\n",
    "            plt.xlabel(f\"log+1({col})\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.show()\n",
    "            plt.savefig(path + '//' + f\"hist_log+1{col}.pdf\", dpi = 600)\n",
    "            plt.close()\n",
    "            print(\"hist_done!\")\n",
    "            plt.figure()\n",
    "            scipy.stats.probplot(frame[col], dist=\"norm\", plot=plt)\n",
    "            plt.title(f\"Q-Q-{col}\")\n",
    "            plt.show\n",
    "            plt.savefig(path + '//' + f\"Q-Q_log+1{col}.pdf\", dpi = 600)\n",
    "            plt.close()\n",
    "            print(\"qq_done:)\")\n",
    "            plt.figure()\n",
    "            sns.distplot(frame[col], kde = True, color ='red', bins = 30)\n",
    "            plt.title(f\"dist-{col}\")\n",
    "            plt.show\n",
    "            plt.savefig(path + '//' + f\"dist_log+1{col}.pdf\", dpi = 600)\n",
    "            plt.close()\n",
    "            print(\"on it:)\")\n",
    "            print(\"done:)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9315bab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cor function\n",
    "def cor_function(list):\n",
    "      for  x in list:\n",
    "        cor = x.corr(method = 'pearson').round(2)\n",
    "        sns.heatmap(cor, annot=True, vmax=1, vmin=-1, center=0, cmap='vlag')\n",
    "        sns.set_style(\"dark\")\n",
    "        cax = plt.gcf().axes[0]\n",
    "        cax.tick_params(labelsize=10)\n",
    "        plt.yticks(rotation=\"horizontal\", fontsize=6)\n",
    "        plt.xticks(rotation=\"vertical\", fontsize=6)\n",
    "        plt.margins(0.2)\n",
    "        plt.subplots_adjust(bottom=0.4)\n",
    "        name = [i for i in globals() if globals()[i] is x][0]\n",
    "        plt.savefig(path + '\\\\' + '{}pearsonheatmap2_full-tryP.pdf'.format(name))\n",
    "        plt.close()\n",
    "        cor = cor.unstack()\n",
    "        cor = abs(cor)\n",
    "        writer = pd.ExcelWriter(path + '//' + '{}PCORDATA-excel.xlsx'.format(name), engine='xlsxwriter')\n",
    "        cor.to_excel(writer)\n",
    "        writer.save()\n",
    "        print('done:)')\n",
    "        rho = x.corr()\n",
    "        pval = x.corr(method=lambda x, y: pearsonr(x, y)[1]) - np.eye(*rho.shape)\n",
    "        p = pval.applymap(lambda x: ''.join(['*' for t in [0.0001,0.001,0.01,0.1,] if x<=t]))\n",
    "        try_1 = rho.round(2).astype(str) + p\n",
    "        try_1.to_html(path + '\\\\' + '{}try1.html'.format(name))\n",
    "        print('done:)2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4119447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bubble_plot(list):\n",
    "    for x in list:\n",
    "        kwargs = {\n",
    "    'heatmap': {\n",
    "        'vmin': -1,\n",
    "        'vmax': 1,\n",
    "        'cmap': 'viridis',\n",
    "    },\n",
    "    'figure': {\n",
    "        'figsize': (12,8),\n",
    "    },\n",
    "    }\n",
    "        name = [i for i in globals() if globals()[i] is x][0]\n",
    "        plot_correlation_heatmap(x, bubble=True, annotate=True,textcolors=['white', 'black'],\n",
    "                                 val_fmt='{x:.2f}',  **kwargs)\n",
    "        plt.style.use(\"classic\")\n",
    "        cax = plt.gcf().axes[0]\n",
    "\n",
    "        font = {'family' : 'Arial',\n",
    "        'size'   : 10}\n",
    "\n",
    "        plt.rc('font', **font)\n",
    "\n",
    "        plt.savefig(path + '\\\\' + '{}bubblescatmatrixv2.pdf'.format(name), dpi=600)\n",
    "        plt.show()\n",
    "        print('done:)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412466a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_interval(data):\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    n = len(data)\n",
    "    z_score = 1.96  # 95% confidence interval\n",
    "\n",
    "    lower_bound = mean - (z_score * (std / np.sqrt(n)))\n",
    "    upper_bound = mean + (z_score * (std / np.sqrt(n)))\n",
    "\n",
    "    return lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faec0779",
   "metadata": {},
   "outputs": [],
   "source": [
    "##feild analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9654938",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = ('RESULTS_mito_tool') \n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "    print('Output folder created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab391108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df = pd.read_csv('mito_feild_data.csv')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f6b750",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_sum = df.drop(columns=[\"ep\", \"position\", \"experiment\", \"plate\", \"row\", \"column\", \"id\",\n",
    "                               \"field\", \"case\"])\n",
    "data_for_sum[\"stage_comp\"] = data_for_sum[\"stage\"].astype(str) + data_for_sum[\"compound\"].astype(str) \n",
    "data_for_sum = data_for_sum.drop(columns=[\"stage\", \"compound\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf16ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_sum.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51f6684",
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary per feild\n",
    "summary_df = summarize_dataframe(data_for_sum,'stage_comp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "71df1bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = 'feild-summary_mito.csv'\n",
    "summary_df.to_csv(file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ed693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test dist per feild\n",
    "data_for_dist = data_for_sum.set_index('stage_comp')\n",
    "dist_check(data_for_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e213e86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clear low and high cell count based on feilds\n",
    "cell_count =  df[\"Cell Count\"].values\n",
    "sns.histplot(cell_count)\n",
    "plt.show()\n",
    "df = df.loc[cell_count<60]\n",
    "sns.histplot(df[\"Cell Count\"].values)\n",
    "plt.show()\n",
    "df = df.loc[cell_count>5]\n",
    "sns.histplot(df[\"Cell Count\"].values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b51e63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##save new frame\n",
    "file2 = 'field-cell_count_filt.csv'\n",
    "df.to_csv(file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019aea15",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create well column\n",
    "df[\"pos\"] = df[\"row\"].astype(str) + df[\"column\"].astype(str)  + df[\"experiment\"].astype(str) + df[\"plate\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44152076",
   "metadata": {},
   "outputs": [],
   "source": [
    "##prepare data for QC cleanup\n",
    "data_for_out = df.drop(columns=[\"ep\", \"position\", \"plate\", \"row\", \"column\",\n",
    "                               \"case\"])\n",
    "data_for_out.assign(FC='1')\n",
    "data_for_out['FC'] = '1'\n",
    "data_for_out['FC'] = data_for_out['FC'].astype(int)\n",
    "data_for_out = data_for_out.set_index(['stage', 'compound', 'experiment', 'pos', 'id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec95d902",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTION1\n",
    "z_out = compute_scores(data_for_out, \"out1_try.csv\")\n",
    "file3 = 'z_out_feild_mdivi.csv'\n",
    "z_out.to_csv(file3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a37a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_out = z_out.reset_index()\n",
    "by_well = z_out.groupby(['pos','stage', 'compound',\n",
    "                             'id'],as_index=False).agg({'network_average_size':'sum','rounded_count':'sum',\n",
    "                                                                'rounded_total_area':'sum', 'rod_average_size':'sum',\n",
    "                                                                 'rod_total_area':'sum', 'rod_count':'sum', 'rod_average_size':'sum',\n",
    "                                                                 'network_total_area':'sum', 'network_count': 'sum',\n",
    "                                                                 'cell_count':'sum', 'field':'sum', 'FC':'sum'\n",
    "                                                                \n",
    "                                                             })\n",
    "file4 = 'bywell_z_out_feild_mdivi.csv'\n",
    "by_well.to_csv(file4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4fd2dbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTION2\n",
    "iqr_out = compute_iqr(data_for_out, \"out2_try.csv\")\n",
    "file5 = 'iqr_out_feild_mdivi.csv'\n",
    "iqr_out.to_csv(file5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "39a31ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "iqr_out = iqr_out.reset_index()\n",
    "by_well2 = iqr_out.groupby(['pos','stage', 'compound',\n",
    "                             'id'],as_index=False).agg({'network_average_size':'sum','rounded_count':'sum',\n",
    "                                                                'rounded_total_area':'sum', 'rod_average_size':'sum',\n",
    "                                                                 'rod_total_area':'sum', 'rod_count':'sum', 'rod_average_size':'sum',\n",
    "                                                                 'network_total_area':'sum', 'network_count': 'sum',\n",
    "                                                                 'cell_count':'sum', 'field':'sum', 'FC':'sum'\n",
    "                                                                \n",
    "                                                             })\n",
    "file6 = 'bywell_iqr_out_feild_mdivi.csv'\n",
    "by_well2.to_csv(file6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b5fbf36f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#OPTION3\n",
    "df_perc = compute_percentiles(data_for_out, \"final-mdivi_perc-out3_try.csv\")\n",
    "file7 = 'final-perc_out_feild_mdivi-new.csv'\n",
    "df_perc.to_csv(file7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "db5d9024",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_perc = df_perc.reset_index()\n",
    "by_well3 = df_perc.groupby(['pos','stage', 'compound', 'experiment',\n",
    "                             'id'],as_index=False).agg({'network_average_size':'sum','rounded_count':'sum', 'rounded_average_size':'sum',\n",
    "                                                                'rounded_total_area':'sum', 'rod_average_size':'sum',\n",
    "                                                                 'rod_total_area':'sum', 'rod_count':'sum', 'rod_average_size':'sum',\n",
    "                                                                 'network_total_area':'sum', 'network_count': 'sum',\n",
    "                                                                 'cell_count':'sum', 'field':'sum', 'FC':'sum'\n",
    "                                                                \n",
    "                                                             })\n",
    "file8 = 'bywell_perc_out_feild_mdivi.csv'\n",
    "by_well3.to_csv(file8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de311ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##continue data analysis per wells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d62bf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = ('mdivi_batch_data_prep') \n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "    print('Output folder created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bda571",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('adam_mdivi/mdivi_well_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04177fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check feild per well dist\n",
    "df_feild = df[[\"FC\"]]\n",
    "dist_check(df_feild)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8257ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##summary stats per FC\n",
    "df_for_sum = df.drop(columns=[\"pos\", \"stage\", \"compound\", \"id\", \"experiment\",\n",
    "                               ])\n",
    "s_df = summarize_dataframe(df_for_sum, \"FC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ebae5db0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file1 = 'summary1_well_mdivi.csv'\n",
    "s_df.to_csv(file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ed5639f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.assign(WC='1')\n",
    "df['WC'] = '1'\n",
    "df['WC'] = df['WC'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2a1a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear low feild count\n",
    "feild_count =  df[\"FC\"].values\n",
    "sns.histplot(feild_count)\n",
    "plt.show()\n",
    "df = df.loc[feild_count<23]\n",
    "sns.histplot(df[\"FC\"].values)\n",
    "plt.show()\n",
    "df = df.loc[feild_count>5]\n",
    "sns.histplot(df[\"FC\"].values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a652414b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##prepare data fro batch analysis\n",
    "df[\"sample\"] = df[\"stage\"].astype(str) + df[\"id\"].astype(str)  \n",
    "df = df.drop(columns=[\"field\", \"Unnamed: 0\",  \"pos\"])\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0edcda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##batch frame \n",
    "data_for_batch = df.groupby(['sample', 'experiment', 'compound', 'stage', 'id'\n",
    "                             ],as_index=False).agg({'network_average_size':'mean',\n",
    "                                                    'network_count':'mean',\n",
    "                                                    'network_total_area':'mean',\n",
    "                                                    'rod_average_size':'mean', \n",
    "                                                    'rod_count':'mean',\n",
    "                                                    'rod_total_area':'mean',\n",
    "                                                    'rounded_count':'mean', 'rounded_total_area':'mean',\n",
    "                                                    'rounded_average_size': 'mean',\n",
    "                                                    'WC':'sum', 'cell_count':'mean',  'FC':'mean'\n",
    "                                                                \n",
    "                                                             })\n",
    "file2 = 'data_for_batch_mdivi.csv'\n",
    "data_for_batch.to_csv(file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a538fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output folder created\n"
     ]
    }
   ],
   "source": [
    "path = ('Results_mdivi_analysis') \n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "    print('Output folder created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe75794",
   "metadata": {},
   "outputs": [],
   "source": [
    "#next step read - feature interaction for selection\n",
    "df = pd.read_csv('mito_clean_well_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c8846a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Sample\"] = df[\"stage\"].astype(str) + df[\"id\"].astype(str) \n",
    "df.set_index([\"Sample\"], inplace = True,\n",
    "                             append = True, drop = False)\n",
    "df.fillna(0, inplace=True)\n",
    "df.set_index([\"Sample\"], inplace = True,\n",
    "                             append = True, drop = False)\n",
    "df.fillna(0, inplace=True)\n",
    "df2 = df.groupby(level=1).mean()\n",
    "df_clean = df2.drop(columns=['cell_count', 'fields'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6b430ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bff3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalbymean=(df_clean-df_clean.mean())/df_clean.std()\n",
    "df_max_scaled = df_clean.copy()\n",
    "for column in df_max_scaled.columns:\n",
    "    df_max_scaled[column] = df_max_scaled[column]  / df_max_scaled[column].abs().max()\n",
    "df_min_max_scaled = df_clean.copy()\n",
    "for column in df_min_max_scaled.columns:\n",
    "    df_min_max_scaled[column] = (df_min_max_scaled[column] - df_min_max_scaled[column].min()) / (df_min_max_scaled[column].max() - df_min_max_scaled[column].min())   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "64ad027f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corleation_list_all = [(df_normalbymean), (df_max_scaled), (df_min_max_scaled)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1a566a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bubble_plot(corleation_list_all)\n",
    "cor_function(corleation_list_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7dc7a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "cor = df_normalbymean.corr(method = 'pearson').round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a45d166",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.pairplot(df_normalbymean, kind=\"reg\")\n",
    "plt.savefig(path + '\\\\' + 'highlightpairplot.pdf', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f3571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## well QC steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca08b06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('mito_data_cleanup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997949ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAN_SCAT_HEAT_DATA = df.copy()\n",
    "CLEAN_SCAT_HEAT_DATA = CLEAN_SCAT_HEAT_DATA.drop(columns=['ep', 'experiment', 'plate', 'fields', 'column', 'row'])\n",
    "CLEAN_SCAT_HEAT_DATA[\"group\"] = CLEAN_SCAT_HEAT_DATA[\"stage\"].astype(str) \n",
    "CLEAN_SCAT_HEAT_DATA.set_index([\"group\"], inplace = True,\n",
    "                            append = True, drop = False)\n",
    "CLEAN_SCAT_HEAT_DATA[\"group_with_id\"] = CLEAN_SCAT_HEAT_DATA[\"stage\"].astype(str) + CLEAN_SCAT_HEAT_DATA[\"id\"].astype(str) \n",
    "CLEAN_SCAT_HEAT_DATA.set_index([\"group_with_id\"], inplace = True,\n",
    "                            append = True, drop = False)\n",
    "CLEAN_SCAT_HEAT_DATA[\"group_with_pc\"] = CLEAN_SCAT_HEAT_DATA[\"stage\"].astype(str) + CLEAN_SCAT_HEAT_DATA[\"id\"].astype(str) + CLEAN_SCAT_HEAT_DATA[\"pos\"].astype(str)\n",
    "CLEAN_SCAT_HEAT_DATA.set_index([\"group_with_pc\"], inplace = True,\n",
    "                            append = True, drop = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2700c60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAN_SCAT_HEAT_DATA[\"group\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61d0a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear low and high cell count\n",
    "cell_count =  CLEAN_SCAT_HEAT_DATA[\"cell_count\"].values\n",
    "sns.histplot(cell_count)\n",
    "plt.show()\n",
    "CLEAN_SCAT_HEAT_DATA = CLEAN_SCAT_HEAT_DATA.loc[cell_count<700]\n",
    "sns.histplot(CLEAN_SCAT_HEAT_DATA[\"cell_count\"].values)\n",
    "plt.show()\n",
    "CLEAN_SCAT_HEAT_DATA = CLEAN_SCAT_HEAT_DATA.loc[cell_count>250]\n",
    "sns.histplot(CLEAN_SCAT_HEAT_DATA[\"cell_count\"].values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2e5f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_1 =   'df1_ORIGINAL_det_mito_clean.csv'\n",
    "CLEAN_SCAT_HEAT_DATA.to_csv(file_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e23f01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove unwanted feat&keep features by corelation analysis\n",
    "CLEAN_SCAT_HEAT_DATA = CLEAN_SCAT_HEAT_DATA.drop(columns=['rounded_fraction', 'rod_fraction', 'network_fraction',\n",
    "                                                         'pos', 'stage', 'id', 'cell_count',\n",
    "                                                         'network_count', 'network_total_area', 'rod_count', 'rod_total_area',\n",
    "                                                         'rounded_count', 'rounded_total_area', 'mitochonria_count_per_cell',\n",
    "                                                         ])\n",
    "df_clean = CLEAN_SCAT_HEAT_DATA[['group', 'group_with_id', \"group_with_pc\",\n",
    "                           \"network_area_per_cell\", \"network_count_per_cell\", \"network_average_size\",\n",
    "                          \"rod_average_size\", \"rod_area_per_cell\", \"rod_count_per_cell\",\n",
    "                          \"rounded_average_size\", \"rounded_area_per_cell\", \"rounded_count_per_pell\",\n",
    "                          \"mitochondria_area_per_cell\"]]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564c1bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_2 =   'df2_ORIGINAL_det_mito_clean.csv'\n",
    "df_clean.to_csv(file_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7886a5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f32c3dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Severe' 'HC' 'PreManifest' 'Mild']\n"
     ]
    }
   ],
   "source": [
    "# OUTLIER DETECTION MODE \n",
    "DataFrame_OutliersDetections = df_clean.copy()\n",
    "types = df_clean[\"group\"].unique()\n",
    "mask = []\n",
    "features = df_clean.columns[4:13]\n",
    "print(types)\n",
    "\n",
    "\n",
    "detector_list = [\n",
    "    \n",
    "        (\n",
    "        \"Local Outlier Factor 8\",\n",
    "        LocalOutlierFactor(n_neighbors=8),\n",
    "    ),\n",
    "    (\n",
    "        \"Local Outlier Factor 5\",\n",
    "        LocalOutlierFactor(n_neighbors=5),\n",
    "    ),\n",
    "        (\n",
    "         \"Local Outlier Factor 30\",\n",
    "        LocalOutlierFactor(n_neighbors=30),\n",
    "        ),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7aa261b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, algorithm in detector_list:\n",
    "    errors = np.full(len(DataFrame_OutliersDetections),fill_value=np.nan)\n",
    "    outliers = np.full(len(DataFrame_OutliersDetections),fill_value=np.nan)\n",
    "\n",
    "    for type in types:\n",
    "        x = DataFrame_OutliersDetections.loc[:,features].values\n",
    "        F = x.sum(1)\n",
    "        mask = np.zeros(x.shape[0])\n",
    "        mask[np.isfinite(F)] = 1\n",
    "        mask_type = mask * np.array(DataFrame_OutliersDetections[\"group\"] == type)\n",
    "        Curr_df = DataFrame_OutliersDetections.loc[mask_type==1,features]\n",
    "\n",
    "        x = Curr_df.values\n",
    "        if name == 'pca_approx':\n",
    "\n",
    "            x = StandardScaler().fit_transform(x)\n",
    "\n",
    "            lower_dimensional_data = algorithm.fit_transform(x)\n",
    "            pproximation = algorithm.inverse_transform(lower_dimensional_data)\n",
    "\n",
    "            err = np.linalg.norm(x-pproximation,2,axis=1)\n",
    "\n",
    "            errors[mask_type==1] = err\n",
    "            outliers[mask_type==1] = (err < 5) * 2 - 1\n",
    "            if False:\n",
    "                plt.scatter(lower_dimensional_data[:,0],lower_dimensional_data[:,1],c=err>5)\n",
    "                plt.title(type)\n",
    "                plt.show()\n",
    "        else:\n",
    "            algorithm.fit(x)\n",
    "            if name == \"Robust covariance\":\n",
    "                errors[mask_type==1] = algorithm.mahalanobis(x)\n",
    "                outliers[mask_type==1] = algorithm.predict(x)\n",
    "\n",
    "            if  \"Local Outlier Factor\" in name:\n",
    "                errors[mask_type==1] = algorithm.negative_outlier_factor_\n",
    "                outliers[mask_type==1] = algorithm.fit_predict(x)\n",
    "            else:\n",
    "                y_pred = algorithm.fit(x).predict(x)\n",
    "\n",
    "\n",
    "\n",
    "    DataFrame_OutliersDetections[name] = errors\n",
    "    DataFrame_OutliersDetections[f'{name}_outliers'] = outliers\n",
    "    DataFrame_OutliersDetections.set_index(name, inplace = True,\n",
    "                            append = True, drop = False)\n",
    "\n",
    "    file3 =   'df3_ORIGINAL_det_mito_clean.csv'\n",
    "    DataFrame_OutliersDetections.to_csv(file3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f55c659",
   "metadata": {},
   "outputs": [],
   "source": [
    "##keep good wells\n",
    "df_clean = DataFrame_OutliersDetections[DataFrame_OutliersDetections['Local Outlier Factor 30_outliers'] == 1]\n",
    "file4 = 'df4_ORIGINAL_det_mito_clean.csv'\n",
    "df_clean.to_csv(file4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7165c9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MiguelW12\\AppData\\Local\\Temp\\ipykernel_20864\\4204758423.py:3: FutureWarning:\n",
      "\n",
      "The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#define hc  mean data frames \n",
    "hc = df_clean[(df_clean['group'] == 'HC')]\n",
    "hc_mean = hc.mean()\n",
    "Nh_df = df_clean/hc_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3523333d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log = df_clean.drop(columns=[\"group_with_pc\", \"group\", \"group_with_id\",\n",
    "                           'Local Outlier Factor 8',\n",
    "       'Local Outlier Factor 8_outliers', 'Local Outlier Factor 5',\n",
    "       'Local Outlier Factor 5_outliers', 'Local Outlier Factor 30',\n",
    "       'Local Outlier Factor 30_outliers'])\n",
    "for col in df_log:\n",
    "    df_log[col] = np.log(df_log[col])\n",
    "    NL_df = df_log.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "870a96a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log_2 = df_clean.drop(columns=[\"group_with_pc\", \"group\", \"group_with_id\",\n",
    "                           'Local Outlier Factor 8',\n",
    "       'Local Outlier Factor 8_outliers', 'Local Outlier Factor 5',\n",
    "       'Local Outlier Factor 5_outliers', 'Local Outlier Factor 30',\n",
    "       'Local Outlier Factor 30_outliers'])\n",
    "for col in df_log_2:\n",
    "        df_log_2[col] = np.log(1+df_log_2[col]-min(df_log_2[col]))\n",
    "        NL2_df = df_log_2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4fe8a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_max_scaled = df_clean.drop(columns=[\"group_with_pc\", \"group\", \"group_with_id\",\n",
    "                           'Local Outlier Factor 8',\n",
    "       'Local Outlier Factor 8_outliers', 'Local Outlier Factor 5',\n",
    "       'Local Outlier Factor 5_outliers', 'Local Outlier Factor 30',\n",
    "       'Local Outlier Factor 30_outliers'])\n",
    "for column in df_max_scaled.columns:\n",
    "        df_max_scaled[column] = df_max_scaled[column]  / df_max_scaled[column].abs().max()\n",
    "        NM_df = df_max_scaled.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4e8c8167",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MiguelW12\\AppData\\Local\\Temp\\ipykernel_15220\\393893059.py:13: FutureWarning:\n",
      "\n",
      "save is not part of the public API, usage can give unexpected results and will be removed in a future version\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#initialze the excel writer\n",
    "writer = pd.ExcelWriter(path + '//' + 'normalzied-excel.xlsx', engine='xlsxwriter')\n",
    "\n",
    "frames = {'hc': Nh_df, 'log': NL_df, 'log2':NL2_df,\n",
    "              'max': NM_df}\n",
    "\n",
    "for sheet, frame in  frames.items():\n",
    "    frame.to_excel(writer, sheet_name = sheet)\n",
    "\n",
    "writer.save()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5549744a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##feature qc \n",
    "df_var_calc = df_clean.copy()\n",
    "df_var_calc = df_var_calc.drop(columns=['Local Outlier Factor 30', 'Local Outlier Factor 30_outliers',\n",
    "       'Local Outlier Factor 5', 'Local Outlier Factor 5_outliers',\n",
    "       'Local Outlier Factor 8', 'Local Outlier Factor 8_outliers', 'group',\n",
    "       'group_with_id', 'group_with_pc'])\n",
    "df_var_calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74a1a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#variance of each feature\n",
    "feature_variances = df_var_calc.var()\n",
    "\n",
    "#standard deviation of each feature\n",
    "feature_std_devs = df_var_calc.std()\n",
    "\n",
    "#coefficient of variation (CV) for each feature,\n",
    "feature_cv = df_var_calc.std() / df_var_calc.mean()\n",
    "\n",
    "# Print the results\n",
    "print(\"Feature Variance:\")\n",
    "print(feature_variances)\n",
    "\n",
    "print(\"\\nFeature Standard Deviation:\")\n",
    "print(feature_std_devs)\n",
    "\n",
    "print(\"\\nFeature Coefficient of Variation (CV):\")\n",
    "print(feature_cv)\n",
    "\n",
    "\n",
    "feature_variances.to_csv('feature_variances.csv')\n",
    "feature_std_devs.to_csv('feature_std_devs.csv')\n",
    "feature_cv.to_csv('feature_coefficient_of_variation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360f0ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "# Plot feature variances\n",
    "plt.bar(range(len(feature_variances)), feature_variances, align='center')\n",
    "plt.xticks(range(len(feature_variances)), feature_variances.index, rotation=45)\n",
    "plt.title('Feature Variance')\n",
    "plt.ylabel('Variance')\n",
    "plt.xlabel('features')\n",
    "plt.show()\n",
    "plt.savefig(path + '//' + \"feature_var.pdf\", dpi = 600)\n",
    "plt.close()\n",
    "print(\"var_done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5210d88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature standard deviations\n",
    "plt.bar(range(len(feature_std_devs)), feature_std_devs, align='center')\n",
    "plt.xticks(range(len(feature_std_devs)), feature_std_devs.index, rotation=45)\n",
    "plt.title('Feature Standard Deviation')\n",
    "plt.ylabel('Standard Deviation')\n",
    "plt.xlabel('features')\n",
    "plt.show()\n",
    "plt.savefig(path + '//' + \"feature_sd.pdf\", dpi = 600)\n",
    "plt.close()\n",
    "print(\"sd_done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d14e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature coefficient of variation (CV)\n",
    "plt.bar(range(len(feature_cv)), feature_cv, align='center')\n",
    "plt.xticks(range(len(feature_cv)), feature_cv.index, rotation=45)\n",
    "plt.title('Feature Coefficient of Variation (CV)')\n",
    "plt.ylabel('Coefficient of Variation')\n",
    "plt.xlabel('features')\n",
    "plt.show()\n",
    "plt.savefig(path + '//' + \"feature_cv.pdf\", dpi = 600)\n",
    "plt.close()\n",
    "print(\"cv_done!\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6187c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### make sure to groupby mean before plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3aeef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = df_clean.groupby(level=2).mean()\n",
    "df_plot = df_plot.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebac9da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_plot:\n",
    "    \n",
    "    fig = px.bar(df_plot, x = \"group_with_id\", y = col,\n",
    "            barmode = 'group')\n",
    "    fig.write_image(path + '//' + f\"{col}mid_plot_image.pdf\", engine=\"kaleido\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d532f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_check_if_hc(Nh_df)\n",
    "dist_check_if_log(df_log)\n",
    "dist_check_if_log_2(df_log_2)\n",
    "dist_check_if_max(df_max_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b76899f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## choose norm and upload for analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed84f52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output folder created\n"
     ]
    }
   ],
   "source": [
    "path = ('mito_upstream') \n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "    print('Output folder created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a330f5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##fract calc and vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9316ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('adam_sep_final/fraction_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d059c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "##fraction plots+CIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ec6205",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute CI for fraction\n",
    "confidence_intervals = df2.groupby('group').agg({\n",
    "    'network_fraction': confidence_interval,\n",
    "    'rod_fraction': confidence_interval,\n",
    "    'rounded_fraction':confidence_interval\n",
    "}).rename(columns={\n",
    "    'network_fraction': ('network_fraction_lower', 'network_fraction_upper'),\n",
    "    'rod_fraction': ('rod_fraction_lower', 'rod_fraction_upper'),\n",
    "    'rounded_fraction': ('rounded_fraction_lower', 'rounded_fraction_upper')\n",
    "    \n",
    "}).reset_index()\n",
    "\n",
    "df_new = pd.merge(df2, confidence_intervals, on='group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39855459",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new[\"group\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee0e356",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_new.groupby('group', as_index=False).mean()\n",
    "df_new.to_csv('group_fraction_cis.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3e6b3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fract_ci = pd.read_csv('adam_fraction_ci.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6625c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fract_ci.set_index([\"group\"], inplace = True,\n",
    "                           append = True, drop = True)\n",
    "df_fract_ci = df_fract_ci *100\n",
    "df_fract_ci = df_fract_ci.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece89d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fig\n",
    "fig = go.Figure()\n",
    "\n",
    "for feature in [\"network_fraction\", \"rod_fraction\", \"rounded_fraction\"]:\n",
    "    feature_lower_col = f\"{feature}_lower\"\n",
    "    feature_upper_col = f\"{feature}_upper\"\n",
    "   \n",
    "    fig.add_trace(go.Bar(\n",
    "        x=df_fract_ci[\"group\"],\n",
    "        y=df_fract_ci[f'{feature}'],\n",
    "        name=feature,\n",
    "        text=None,  \n",
    "        error_y=dict(\n",
    "            type='data',\n",
    "            symmetric=False,\n",
    "            array=df_fract_ci[feature_upper_col] - df_fract_ci[f'{feature}'],\n",
    "            arrayminus=df_fract_ci[f'{feature}'] - df_fract_ci[feature_lower_col],\n",
    "            visible=True,\n",
    "            thickness=1,\n",
    "            color = \"purple\",\n",
    "           \n",
    "\n",
    "        ),\n",
    "        hoverinfo='text+name',\n",
    "    ))\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Title\",\n",
    "    xaxis=dict(categoryorder='array', categoryarray=['HC', 'PreManifest', 'Mild', 'Severe']),\n",
    "    barmode='stack', \n",
    ")\n",
    "\n",
    "fig.write_image(path + '//' + \"_plot.pdf\", engine=\"kaleido\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfcd9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PROCCESING AND FINAL STATS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750c1409",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('adam_may_final_frame/final_frame_log.csv')\n",
    "df[\"group\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d05379e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats = df.copy()\n",
    "df_stats.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3614cf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "### box plots all feat\n",
    "for col in df_stats.columns[3:]:\n",
    "    \n",
    "    fig = px.box(df_stats, x = \"group\", y = col, \n",
    "                 points=\"all\", color=\"group\",\n",
    "            notched=True)\n",
    "    fig.update_xaxes(categoryorder='array', categoryarray= ['HC', 'PreManifest', 'Mild', 'Severe'])\n",
    "    fig.update_traces(quartilemethod=\"inclusive\") # or \"inclusive\", or \"linear\" by default\n",
    "    fig.update_layout(\n",
    "    font_family=\"Arial\",\n",
    "    font_color=\"Black\",\n",
    "    font_size=20,\n",
    "    font=dict(\n",
    "        family=\"Arial\",\n",
    "        size=20,\n",
    "        color=\"Black\"\n",
    "    )\n",
    ")\n",
    "\n",
    "    fig.write_image(path + '//' + f\"{col}box_plot_image.pdf\", engine=\"kaleido\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cad5558",
   "metadata": {},
   "outputs": [],
   "source": [
    "### move on from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fd0cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#indexing for multivariate\n",
    "df[\"index\"] = df[\"DS\"].astype(str) + df[\"group\"].astype(str) \n",
    "df[\"index2\"] = df[\"DS\"].astype(str) + df[\"group_with_id\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becf3ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA+cor analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa1e7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_DATA_CLEAN = df.copy()\n",
    "PCA_DATA_CLEAN = PCA_DATA_CLEAN.groupby('index2', as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d54676cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a5a41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = PCA_DATA_CLEAN.columns[1:]\n",
    "x = PCA_DATA_CLEAN.loc[:, features].values\n",
    "y = PCA_DATA_CLEAN.loc[:,['index2']].values\n",
    "x = StandardScaler().fit_transform(x)\n",
    "pd.DataFrame(data = x, columns = features).head()\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "PCA_DATA_COMP = pd.DataFrame(data = principalComponents\n",
    "               , columns = ['principal component 1', 'principal component 2'])\n",
    "importances = pd.DataFrame(data = abs(pca.components_).transpose(), columns = ['PC1', 'PC2'])\n",
    "importances['Feature'] = features\n",
    "display(importances.sort_values('PC1', ascending = False).reset_index(drop = True))\n",
    "imp = importances.sort_values('PC2', ascending = False)[0:30]\n",
    "ax1 = sns.barplot(x = 'PC2', y = 'Feature', data = imp)\n",
    "  \n",
    "fig = ax1.get_figure()\n",
    "fig.set_size_inches(4, 8)\n",
    "ax1.set_xticklabels(ax1.get_xticklabels(), rotation = 90) \n",
    "plt.savefig(path + '\\\\' + 'pca1importancefeat1.pdf', dpi=600 )\n",
    "plt.show()\n",
    "imp2 = importances.sort_values('PC1', ascending = False)[0:30]\n",
    "ax2 = sns.barplot(x = 'PC1', y = 'Feature', data = imp2)\n",
    "fig = ax2.get_figure()\n",
    "fig.set_size_inches(4, 8)\n",
    "ax2.set_xticklabels(ax2.get_xticklabels(), rotation = 90)\n",
    "plt.savefig(path + '\\\\' + 'pca1importancefeat2.pdf', dpi=600 )\n",
    "plt.show()\n",
    "\n",
    "PCA_DATA_FINAL = pd.concat([PCA_DATA_COMP, PCA_DATA_CLEAN['index2']], axis = 1)\n",
    "PCA_FIG = plt.figure(figsize = (8,8))\n",
    "ax1 = PCA_FIG.add_subplot(1,1,1) \n",
    "ax1.set_xlabel('PC1', fontsize = 20, fontname=\"Arial\")\n",
    "ax1.set_ylabel('PC2', fontsize = 20, fontname=\"Arial\")\n",
    "ax1.tick_params(axis='x', labelsize=10)\n",
    "ax1.tick_params(axis='y', labelsize=10)\n",
    "for tick in ax1.get_xticklabels():\n",
    "    tick.set_fontname(\"Arial\")\n",
    "for tick in ax1.get_yticklabels():\n",
    "    tick.set_fontname(\"Arial\")\n",
    "\n",
    "types = PCA_DATA_FINAL[\"index2\"].unique()\n",
    "targets1 = [index for index in types]\n",
    "reds = list( Color(\"red\").range_to(Color(\"white\"),6))\n",
    "greens = list( Color(\"green\").range_to(Color(\"white\"),6))\n",
    "blues = list( Color(\"blue\").range_to(Color(\"white\"),6))\n",
    "\n",
    "for target in targets1:\n",
    "    if 'control' == target[:6]:\n",
    "        color = blues[0] \n",
    "    elif 'ds' in target[:2]:\n",
    "        color = reds[0] \n",
    "    else:\n",
    "        color = greens[0]\n",
    "    if 'Severe' in target:\n",
    "        alpha = 1\n",
    "    elif 'Pre' in target:\n",
    "        alpha = 0.5    \n",
    "    else:\n",
    "        alpha = 0.2\n",
    "        \n",
    "    \n",
    "\n",
    "    indicesToKeep = PCA_DATA_FINAL['index2'] == target\n",
    "    ax1.scatter(PCA_DATA_FINAL.loc[indicesToKeep, 'principal component 1']\n",
    "            , PCA_DATA_FINAL.loc[indicesToKeep, 'principal component 2']\n",
    "            , c = mpl.colors.to_hex(str(color)), alpha=alpha\n",
    "            , s = 50)\n",
    "scale = 6    \n",
    "ax1.legend(targets1*scale,fontsize=6)\n",
    "ax1.grid()\n",
    "\n",
    "\n",
    "\n",
    "print(pca.explained_variance_ratio_)\n",
    "plt.savefig(path + '\\\\' + 'pca3.pdf', dpi=600 )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b331d3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab40b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_DATA_CLEAN = df.copy()\n",
    "PCA_DATA_CLEAN = PCA_DATA_CLEAN.groupby('index', as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8202c613",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = PCA_DATA_CLEAN.columns[1:]\n",
    "x = PCA_DATA_CLEAN.loc[:, features].values\n",
    "y = PCA_DATA_CLEAN.loc[:,['index']].values\n",
    "x = StandardScaler().fit_transform(x)\n",
    "pd.DataFrame(data = x, columns = features).head()\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "PCA_DATA_COMP = pd.DataFrame(data = principalComponents\n",
    "               , columns = ['principal component 1', 'principal component 2'])\n",
    "importances = pd.DataFrame(data = abs(pca.components_).transpose(), columns = ['PC1', 'PC2'])\n",
    "importances['Feature'] = features\n",
    "display(importances.sort_values('PC1', ascending = False).reset_index(drop = True))\n",
    "imp = importances.sort_values('PC2', ascending = False)[0:30]\n",
    "ax1 = sns.barplot(x = 'PC2', y = 'Feature', data = imp)\n",
    "  \n",
    "fig = ax1.get_figure()\n",
    "fig.set_size_inches(4, 8)\n",
    "ax1.set_xticklabels(ax1.get_xticklabels(), rotation = 90) \n",
    "plt.savefig(path + '\\\\' + 'pca1importancefeat1-group.pdf', dpi=600 )\n",
    "plt.show()\n",
    "imp2 = importances.sort_values('PC1', ascending = False)[0:30]\n",
    "ax2 = sns.barplot(x = 'PC1', y = 'Feature', data = imp2)\n",
    "fig = ax2.get_figure()\n",
    "fig.set_size_inches(4, 8)\n",
    "ax2.set_xticklabels(ax2.get_xticklabels(), rotation = 90)\n",
    "plt.savefig(path + '\\\\' + 'pca1importancefeat2-group.pdf', dpi=600 )\n",
    "plt.show()\n",
    "\n",
    "PCA_DATA_FINAL = pd.concat([PCA_DATA_COMP, PCA_DATA_CLEAN['index']], axis = 1)\n",
    "PCA_FIG = plt.figure(figsize = (8,8))\n",
    "ax1 = PCA_FIG.add_subplot(1,1,1) \n",
    "ax1.set_xlabel('PC1', fontsize = 20, fontname=\"Arial\")\n",
    "ax1.set_ylabel('PC2', fontsize = 20, fontname=\"Arial\")\n",
    "ax1.tick_params(axis='x', labelsize=10)\n",
    "ax1.tick_params(axis='y', labelsize=10)\n",
    "for tick in ax1.get_xticklabels():\n",
    "    tick.set_fontname(\"Arial\")\n",
    "for tick in ax1.get_yticklabels():\n",
    "    tick.set_fontname(\"Arial\")\n",
    "\n",
    "types = PCA_DATA_FINAL[\"index\"].unique()\n",
    "targets1 = [index for index in types]\n",
    "reds = list( Color(\"red\").range_to(Color(\"white\"),6))\n",
    "greens = list( Color(\"green\").range_to(Color(\"white\"),6))\n",
    "blues = list( Color(\"blue\").range_to(Color(\"white\"),6))\n",
    "\n",
    "for target in targets1:\n",
    "    if 'control' == target[:6]:\n",
    "        color = blues[0] \n",
    "    elif 'ds' in target[:2]:\n",
    "        color = reds[0] \n",
    "    else:\n",
    "        color = greens[0]\n",
    "    if 'Severe' in target:\n",
    "        alpha = 1\n",
    "    elif 'Pre' in target:\n",
    "        alpha = 0.5    \n",
    "    else:\n",
    "        alpha = 0.2\n",
    "        \n",
    "    \n",
    "\n",
    "    indicesToKeep = PCA_DATA_FINAL['index'] == target\n",
    "    ax1.scatter(PCA_DATA_FINAL.loc[indicesToKeep, 'principal component 1']\n",
    "            , PCA_DATA_FINAL.loc[indicesToKeep, 'principal component 2']\n",
    "            , c = mpl.colors.to_hex(str(color)), alpha=alpha\n",
    "            , s = 50)\n",
    "scale = 6    \n",
    "ax1.legend(targets1*scale,fontsize=6)\n",
    "ax1.grid()\n",
    "\n",
    "\n",
    "\n",
    "print(pca.explained_variance_ratio_)\n",
    "plt.savefig(path + '\\\\' + 'pca_group.pdf', dpi=600 )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af048340",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c162e093",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_DATA_CLEAN = df.copy()\n",
    "PCA_DATA_CLEAN[\"group\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3003e02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = PCA_DATA_CLEAN.loc[:, features].values\n",
    "y = PCA_DATA_CLEAN.loc[:,['group']].values\n",
    "y = np.array([0 if('H' == y_i[0][0:1]) else 1 if  'P' in y_i[0][0:1] else 2 if  'M' in y_i[0][0:1] else 3 for y_i in y])\n",
    "\n",
    "y[0]=2\n",
    "x = StandardScaler().fit_transform(x)\n",
    "\n",
    "lda = LDA(n_components=2)\n",
    "lda_x = np.array(lda.fit(x, y).transform(x))\n",
    "\n",
    "    \n",
    "fig = ax1.get_figure()\n",
    "fig.set_size_inches(4, 8)\n",
    "plt.rc('ytick', labelsize=3)\n",
    "ax1.set_xticklabels(ax1.get_xticklabels(), rotation = 90)  \n",
    "plt.savefig(path + '\\\\' + 'LDA1importancefeat1.pdf', dpi=600)\n",
    "plt.show()\n",
    "imp2 = importances.sort_values('PC1', ascending = False)[0:30]\n",
    "ax2 = sns.barplot(x = 'PC1', y = 'Feature', data = imp2)\n",
    "fig = ax2.get_figure()\n",
    "fig.set_size_inches(4, 8)\n",
    "plt.rc('ytick', labelsize=3)\n",
    "ax2.set_xticklabels(ax2.get_xticklabels(), rotation = 90) \n",
    "plt.savefig(path + '\\\\' + 'LDA1importancefeat2.pdf', dpi=600)\n",
    "plt.show()\n",
    "\n",
    "font = {'family' : 'Arial',\n",
    "        'size'   : 22}\n",
    "\n",
    "plt.rc('font', **font)\n",
    "scale = 6\n",
    "\n",
    "target_names= ['HC', 'PreManifest', 'Mild', 'Severe']\n",
    "plt.figure()\n",
    "plt.figure(figsize = (8,8))\n",
    "for c, i, target_name in zip(\"bgyr\", [0, 1, 2, 3], target_names):\n",
    "    plt.scatter(lda_x[y == i,0] , lda_x[y == i,1] , c=c, label=target_name)\n",
    "    plt.legend(target_names*scale,fontsize=12)\n",
    "#     plt.title('LDA of dataset')\n",
    "    plt.xlabel('LD1', fontsize = 20, family = 'Arial')\n",
    "    plt.ylabel('LD2', fontsize = 20, family = 'Arial')\n",
    "    plt.yticks(family = 'Arial', size = 10)\n",
    "    plt.xticks(family = 'Arial', size= 10)\n",
    "print(lda.explained_variance_ratio_)            \n",
    "plt.savefig(path + '\\\\' + 'LDA.pdf', dpi=600 )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1646d7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##cor-FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1fc021e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cor = df.copy()\n",
    "cor.set_index([\"group_with_id\"], inplace = True,\n",
    "                             append = True, drop = False)\n",
    "cor.fillna(0, inplace=True)\n",
    "cor = cor.groupby(level=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c2cfd53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corleation_list_all = [(cor)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3013d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_function(corleation_list_all)\n",
    "bubble_plot(corleation_list_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaec351",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
